{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c4fcee",
   "metadata": {},
   "source": [
    "# 1. Ponniyin Selvan Word Embeddings using Continuous Bag of Words (CBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76d501",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will train word embeddings using the Continuous Bag of Words (CBoW) model on a Tamil text corpus from the classic work *Ponniyin Selvan* by Kalki Krishnamurthy. Word embeddings are crucial in natural language processing (NLP) because they allow us to represent words as dense vectors that capture semantic relationships between them. \n",
    "\n",
    "**Objective**: We aim to preprocess the Tamil text, tokenize it, and build a CBoW model using Keras to learn meaningful embeddings. These embeddings can later be used for various NLP tasks, such as text classification, sentiment analysis, or even as input features for more complex deep learning models.\n",
    "\n",
    "**Why Use CBoW?**: The CBoW model predicts a target word given its surrounding context words. It is simple yet effective in capturing semantic meanings and is computationally efficient compared to other models like Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1db83",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb418b",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries. These include `re` for regular expression operations, `numpy` for numerical computations, and TensorFlow and Keras for building our neural network model. We also import functions from `indic-nlp-library` to handle tokenization specific to Tamil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d5201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, Reshape\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from indicnlp.tokenize.sentence_tokenize import sentence_split\n",
    "from indicnlp.tokenize import indic_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91228110",
   "metadata": {},
   "source": [
    "## Step 2: Define Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aad420",
   "metadata": {},
   "source": [
    "Next, we define functions to clean the text. The `clean_numbers` function removes digits, and the `clean_special_characters` function removes characters that are not part of the Tamil script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a3d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec7609",
   "metadata": {},
   "source": [
    "## Step 3: Load and Preprocess the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659fa6d",
   "metadata": {},
   "source": [
    "We load the text from the file `Ponniyin Selvan.txt` and perform several preprocessing steps:\n",
    "- Replace punctuation with spaces or periods.\n",
    "- Remove unnecessary characters like quotes and extra spaces.\n",
    "- Clean numbers and special characters.\n",
    "Finally, we split the text into sentences using the `indic-nlp-library`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33dd5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading txt file...\")\n",
    "with open(r'ponniyin-selvan.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocessing: Replacing punctuation and cleaning\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_  \")\n",
    "text = text.replace(\":\", \" _comma_  \")\n",
    "text = text.replace(\";\", \" _comma_  \")\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace('\"', \"\")  # Remove double quotes\n",
    "text = text.replace(\"'\", \"\")  # Remove single quotes\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "text = text.replace('\"', \"\")\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"  \", \" \")\n",
    "text = text.replace(\"\\u200c\", \"\")\n",
    "\n",
    "# Additional cleaning: Remove numbers and combine spaces\n",
    "text = clean_numbers(text)\n",
    "text = clean_roman_numerals(text)\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# Sentence splitting using indic-nlp-library for tamil\n",
    "sentences = sentence_split(text, lang='ta')  # tamil language code\n",
    "\n",
    "# Lowercase and tokenize the sentences\n",
    "sentences = [s.lower().strip() for s in sentences if len(s.split()) > 2]\n",
    "tokenized_sentences = [indic_tokenize.trivial_tokenize(s, lang='te') for s in sentences]\n",
    "\n",
    "print(\"Preprocessing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3772635b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['பொன்னியின் செல்வன் வரலாற்றுப் புதினம் அமரர் கல்கி கிருஷ்ணமூர்த்தி அத்தியாயம் ஆடித்திருநாள் ஆதி அந்தமில்லாத கால வெள்ளத்தில் கற்பனை ஓடத்தில் ஏறி நம்முடன் சிறிது நேரம் பிரயாணம் செய்யுமாறு நேயர்களை அழைக்கிறோம்.',\n",
       " 'விநாடிக்கு ஒரு நூற்றாண்டூ வீதம் எளிதில் கடந்து இன்றைக்குத் தொள்ளாயிரத்து எண்பத்திரண்டூ (ல் எழுதியது) ஆண்டூகளுக்கு முந்திய காலத்துக்குச் செல்வோமாக.',\n",
       " 'தொண்டை நாட்டுக்கும் சோழ நாட்டுக்கும் இடையில் உள்ள திருமுனைப்பாடி நாட்டின் தென்பகுதியில் _comma_ தில்லைச் சிற்றம்பலத்துக்கு மேற்கே இரண்டூ காததூரத்தில் _comma_ அலை கடல் போன்ற ஓர் ஏரி விரிந்து பரந்து கிடக்கிறது.',\n",
       " 'அதற்கு வீரநாராயண ஏரி என்று பெயர்.',\n",
       " 'அது தெற்கு வடக்கில் ஒன்றரைக் காத நீளமும் கிழக்கு மேற்கில் அரைக் காத அகலமும் உள்ளது.',\n",
       " 'காலப்போக்கில் அதன் பெயர் சிதைந்து இந்நாளில் வீராணத்து ஏரி என்ற பெயரால் வழங்கி வருகிறது.',\n",
       " 'புது வெள்ளம் வந்து பாய்ந்து ஏரியில் நீர் நிரம்பித் ததும்பி நிற்கும் ஆடி ஆவணி மாதங்களில் வீரநாராயண ஏரியைப் பார்ப்பவர் எவரும் நம்முடைய பழந்தமிழ் நாட்டு முன்னோர்கள் தங்கள் காலத்தில் சாதித்த அரும்பெரும் காரியங்களைக் குறித்துப் பெருமிதமும் பெரு வியப்பும் கொள்ளாமலிருக்க முடியாது.',\n",
       " 'நம் மூதாதையர்கள் தங்களுடைய நலனுக்கும் தங்கள் காலத்திய மக்களின் நலனுக்கும் உரிய காரியங்களை மட்டூமா செய்தார்கள்.',\n",
       " 'தாய்த் திருநாட்டில் தங்களுக்குப் பிற்காலத்தில் வாழையடி வாழையாக வரப்போகும் ஆயிரங்கால சந்ததிகளுக்கும் நன்மை பயக்கும் மாபெரும் செயல்களை நிறைவேற்றி விட்டுப் போனார்கள் அல்லவா.',\n",
       " 'ஆடித் திங்கள் பதினெட்டாம் நாள் முன் மாலை நேரத்தில் அலை கடல் போல் விரிந்து பரந்திருந்த வீர நாராயண ஏரிக்கரை மீது ஒரு வாலிப வீரன் குதிரை ஏறிப் பிரயாணம் செய்து கொண்டிருந்தான்.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3913306",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551f189",
   "metadata": {},
   "source": [
    "We define the embedding dimensions (`dim`), the context window size (`window_size`), and calculate the vocabulary size (`V`). The vocabulary size is determined by the number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42f1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100  # Embedding dimensions\n",
    "window_size = 2  # Context window size\n",
    "V = len(set(word for sentence in tokenized_sentences for word in sentence)) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf7ab8",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize and Convert Words to Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72317810",
   "metadata": {},
   "source": [
    "We use Keras's `Tokenizer` to convert the words into numerical sequences. This step is crucial for feeding the text data into our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55d7bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_sentences)\n",
    "corpus = tokenizer.texts_to_sequences(tokenized_sentences)\n",
    "V = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b26980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sentences: 52041\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of sentences in the corpus\n",
    "total_sentences = len(corpus)\n",
    "print(f\"Total number of sentences: {total_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4640775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a smaller subset of the corpus for testing\n",
    "num_sentences_to_use = 6000\n",
    "corpus_subset = corpus[:num_sentences_to_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8a75c",
   "metadata": {},
   "source": [
    "## Step 6: Generate Training Data for CBoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c3d96",
   "metadata": {},
   "source": [
    "We create functions to generate training data for the CBoW model. The `generate_data` function yields context words and the target word, while the `generate_all_data_cbow` function generates all the training data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512b716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(corpus, window_size, V):\n",
    "    maxlen = window_size * 2\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            contexts = []\n",
    "            labels = []\n",
    "            s = index - window_size\n",
    "            e = index + window_size + 1\n",
    "            \n",
    "            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
    "            labels.append(word)\n",
    "            \n",
    "            x = pad_sequences(contexts, maxlen=maxlen)\n",
    "            y = to_categorical(labels, V)\n",
    "            yield (x, y)\n",
    "\n",
    "def generate_all_data_cbow(corpus, window_size, V):\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for sentence in corpus:\n",
    "        L = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_words = [sentence[i] if 0 <= i < L and i != index else 0 for i in range(start, end)]\n",
    "            all_in.append(context_words)\n",
    "            all_out.append(to_categorical(word, V))\n",
    "    return np.array(all_in), np.array(all_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e351f8f",
   "metadata": {},
   "source": [
    "## Step 7: Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fde366",
   "metadata": {},
   "source": [
    "We use the `generate_all_data_cbow` function to generate all the training data at once. The `X_cbow` and `y_cbow` arrays contain the input context words and the target words, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d40bb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes - X: (63478, 5), y: (63478, 69351)\n"
     ]
    }
   ],
   "source": [
    "X_cbow, y_cbow = generate_all_data_cbow(corpus_subset, window_size, V)\n",
    "print(f\"Data shapes - X: {X_cbow.shape}, y: {y_cbow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b193c3e2",
   "metadata": {},
   "source": [
    "## Step 8: Build the CBoW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd6d9d",
   "metadata": {},
   "source": [
    "We build a simple CBoW model using Keras. The model consists of an Embedding layer, a Lambda layer to average the context word embeddings, and a Dense output layer with a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d22f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 100)            6935100   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 69351)             7004451   \n",
      "=================================================================\n",
      "Total params: 13,939,551\n",
      "Trainable params: 13,939,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow_model = Sequential()\n",
    "cbow_model.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2, embeddings_initializer='glorot_uniform'))\n",
    "cbow_model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "cbow_model.add(Dense(V, activation='softmax', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "cbow_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24374e",
   "metadata": {},
   "source": [
    "## Step 9: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4959186",
   "metadata": {},
   "source": [
    "We train the model using the `fit` method. The model will be trained for 15 epochs with a batch size of 64. The goal is to minimize the categorical cross-entropy loss and improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e67497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 4) for input Tensor(\"embedding_input:0\", shape=(None, 4), dtype=float32), but it was called on an input with incompatible shape (None, 5).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 4) for input Tensor(\"embedding_input:0\", shape=(None, 4), dtype=float32), but it was called on an input with incompatible shape (None, 5).\n",
      "992/992 [==============================] - 97s 98ms/step - loss: 8.4746 - accuracy: 0.1819\n",
      "Epoch 2/75\n",
      "992/992 [==============================] - 101s 102ms/step - loss: 7.2825 - accuracy: 0.2068\n",
      "Epoch 3/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 7.0758 - accuracy: 0.2186\n",
      "Epoch 4/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 6.9544 - accuracy: 0.2187\n",
      "Epoch 5/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 6.8261 - accuracy: 0.2191\n",
      "Epoch 6/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 6.6789 - accuracy: 0.2238\n",
      "Epoch 7/75\n",
      "992/992 [==============================] - 105s 105ms/step - loss: 6.5273 - accuracy: 0.2283\n",
      "Epoch 8/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 6.3720 - accuracy: 0.2332\n",
      "Epoch 9/75\n",
      "992/992 [==============================] - 105s 105ms/step - loss: 6.2101 - accuracy: 0.2372\n",
      "Epoch 10/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 6.0355 - accuracy: 0.2432\n",
      "Epoch 11/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 5.8438 - accuracy: 0.2501\n",
      "Epoch 12/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 5.6336 - accuracy: 0.2587\n",
      "Epoch 13/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 5.4037 - accuracy: 0.2690\n",
      "Epoch 14/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 5.1577 - accuracy: 0.2807\n",
      "Epoch 15/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 4.8971 - accuracy: 0.2949\n",
      "Epoch 16/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 4.6261 - accuracy: 0.3116\n",
      "Epoch 17/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 4.3498 - accuracy: 0.3309\n",
      "Epoch 18/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 4.0722 - accuracy: 0.3530\n",
      "Epoch 19/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 3.7974 - accuracy: 0.3784\n",
      "Epoch 20/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 3.5286 - accuracy: 0.4055\n",
      "Epoch 21/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 3.2677 - accuracy: 0.4351\n",
      "Epoch 22/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 3.0177 - accuracy: 0.4673 - loss: 3.0176 - accuracy\n",
      "Epoch 23/75\n",
      "992/992 [==============================] - 105s 105ms/step - loss: 2.7793 - accuracy: 0.4994\n",
      "Epoch 24/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 2.5545 - accuracy: 0.5370\n",
      "Epoch 25/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 2.3430 - accuracy: 0.5717\n",
      "Epoch 26/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 2.1454 - accuracy: 0.6051\n",
      "Epoch 27/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 1.9625 - accuracy: 0.6395\n",
      "Epoch 28/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 1.7929 - accuracy: 0.6721\n",
      "Epoch 29/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 1.6371 - accuracy: 0.7013\n",
      "Epoch 30/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 1.4932 - accuracy: 0.7296\n",
      "Epoch 31/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 1.3625 - accuracy: 0.7556\n",
      "Epoch 32/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 1.2426 - accuracy: 0.7777\n",
      "Epoch 33/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 1.1331 - accuracy: 0.7990\n",
      "Epoch 34/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 1.0347 - accuracy: 0.8162\n",
      "Epoch 35/75\n",
      "992/992 [==============================] - 104s 105ms/step - loss: 0.9449 - accuracy: 0.8335\n",
      "Epoch 36/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.8640 - accuracy: 0.8470\n",
      "Epoch 37/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.7916 - accuracy: 0.8599\n",
      "Epoch 38/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.7267 - accuracy: 0.8705\n",
      "Epoch 39/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.6685 - accuracy: 0.8793\n",
      "Epoch 40/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.6164 - accuracy: 0.8875\n",
      "Epoch 41/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.5700 - accuracy: 0.8942\n",
      "Epoch 42/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.5289 - accuracy: 0.9001\n",
      "Epoch 43/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.4921 - accuracy: 0.9054\n",
      "Epoch 44/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.4592 - accuracy: 0.9097\n",
      "Epoch 45/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.4299 - accuracy: 0.9140\n",
      "Epoch 46/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.4036 - accuracy: 0.9175\n",
      "Epoch 47/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.3806 - accuracy: 0.9195\n",
      "Epoch 48/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.3599 - accuracy: 0.9221\n",
      "Epoch 49/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.3409 - accuracy: 0.9245\n",
      "Epoch 50/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 0.3249 - accuracy: 0.9269\n",
      "Epoch 51/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.3099 - accuracy: 0.9291\n",
      "Epoch 52/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.2964 - accuracy: 0.9302\n",
      "Epoch 53/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 0.2843 - accuracy: 0.9324\n",
      "Epoch 54/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.2739 - accuracy: 0.9334\n",
      "Epoch 55/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2642 - accuracy: 0.9347\n",
      "Epoch 56/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2555 - accuracy: 0.9355\n",
      "Epoch 57/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.2474 - accuracy: 0.9368\n",
      "Epoch 58/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2403 - accuracy: 0.9385\n",
      "Epoch 59/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2337 - accuracy: 0.9386\n",
      "Epoch 60/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2278 - accuracy: 0.9396\n",
      "Epoch 61/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 0.2224 - accuracy: 0.9400\n",
      "Epoch 62/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2175 - accuracy: 0.9408\n",
      "Epoch 63/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2126 - accuracy: 0.9419\n",
      "Epoch 64/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2084 - accuracy: 0.9422 - loss: 0.2083 - accuracy: 0.\n",
      "Epoch 65/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.2047 - accuracy: 0.9430\n",
      "Epoch 66/75\n",
      "992/992 [==============================] - 106s 107ms/step - loss: 0.2010 - accuracy: 0.9436\n",
      "Epoch 67/75\n",
      "992/992 [==============================] - 109s 110ms/step - loss: 0.1976 - accuracy: 0.9441\n",
      "Epoch 68/75\n",
      "992/992 [==============================] - 104s 105ms/step - loss: 0.1946 - accuracy: 0.9445\n",
      "Epoch 69/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1917 - accuracy: 0.9444\n",
      "Epoch 70/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1889 - accuracy: 0.9456\n",
      "Epoch 71/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1864 - accuracy: 0.9462\n",
      "Epoch 72/75\n",
      "992/992 [==============================] - 106s 106ms/step - loss: 0.1841 - accuracy: 0.9459\n",
      "Epoch 73/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1820 - accuracy: 0.9462\n",
      "Epoch 74/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1801 - accuracy: 0.9462\n",
      "Epoch 75/75\n",
      "992/992 [==============================] - 105s 106ms/step - loss: 0.1780 - accuracy: 0.9463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2670ccb8940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.fit(X_cbow, y_cbow, batch_size=64, epochs=75, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef9516e-fc4e-4b91-87bd-35ec9c002d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_cbow = len(set(word for sentence in corpus_subset for word in sentence)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77f1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode with utf-8 encoding\n",
    "with open('my_cbow_vectors_ps.txt', 'w', encoding='utf-8') as f:\n",
    "    # Write the header: number of words and the dimension of the vectors\n",
    "    f.write('{} {}\\n'.format(V_cbow - 1, dim))\n",
    "\n",
    "    # Retrieve the word vectors from the model\n",
    "    vectors = cbow_model.get_weights()[0]\n",
    "\n",
    "    # Loop through the word index from the tokenizer\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        # Convert the vector to a string\n",
    "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
    "        # Write the word and its vector to the file\n",
    "        f.write('{} {}\\n'.format(word, str_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c989f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_cbow = gensim.models.KeyedVectors.load_word2vec_format('./my_cbow_vectors_ps.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5c8a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('சோழனும்', 0.6407018899917603),\n",
       " ('நிகர்த்த', 0.4783225953578949),\n",
       " ('கரிகாலர்', 0.4767846465110779),\n",
       " ('கரிகாலன்', 0.4744872450828552),\n",
       " ('கரிகாலனின்', 0.4527541399002075),\n",
       " ('உரியவர்கள்', 0.4411161541938782),\n",
       " ('நடத்திய', 0.43299442529678345),\n",
       " ('கரிகாலனை', 0.4311104416847229),\n",
       " ('தஞ்சைச்', 0.43104785680770874),\n",
       " ('நிறைவேற்றுவதற்கு', 0.42015257477760315)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cbow.most_similar(positive=['சோழன்'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
