{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb9a17f",
   "metadata": {},
   "source": [
    "# 3. Ponniyn Selvan RNN Chatbot with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719874c8",
   "metadata": {},
   "source": [
    "The purpose of the third notebook is to develop our second RNN chatbot model. Unlike in the previous notebook, where we used pre-trained embeddings from the historic Tamil text Ponniyin Selvan, this time we will apply the same preprocessing steps to the data. However, instead of relying on pre-trained CBoW embeddings, we will make the embedding layer of the RNN trainable. This approach will allow the RNN to simultaneously create its own embeddings and train the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c1392",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfb2f77-75fc-4958-8b48-d03e8947dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# For Keras model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "from indicnlp.tokenize.sentence_tokenize import sentence_split\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598c19b0",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef552543",
   "metadata": {},
   "source": [
    "We are going to set up tokens first:\n",
    "\n",
    "- unknown_token: This token will represent words that aren’t in our vocabulary. It helps the model manage unfamiliar words during training and generation.\n",
    "\n",
    "- sentence_start_token: This token will be added to the beginning of every sentence, so the model understands where sentences start.\n",
    "\n",
    "- sentence_end_token: This token will be placed at the end of each sentence, allowing the model to know when the sentence is complete.\n",
    "\n",
    "These tokens will help the model structure sentences and deal with unknown words effectively during training and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ac42e4-4613-4fde-937d-d724609d419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae2275",
   "metadata": {},
   "source": [
    "We want to clean any numbers and roman numerals that might arise in the in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ee8742-6467-457b-a7b2-cbd59d7497d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1003ec",
   "metadata": {},
   "source": [
    "Next, we are going to read and clean the text from The Invisible Man by H.G. Wells. The goal is to prepare the text so it can be fed into our model without unnecessary punctuation, chapter headings, or formatting issues. Here's how we'll do that:\n",
    "\n",
    "- <b>Reading the file:</b> We’ll open the text file (invisible_man_gutenberg.txt) and read its content into memory.\n",
    "\n",
    "- <b>Removing unwanted punctuation:</b>\n",
    "        We'll remove specific punctuation marks, such as commas, colons, quotes, and dashes, by creating a translation table.\n",
    "        We’ll also use regular expressions to keep sentence-ending punctuation like periods (.), question marks (?), and exclamation marks (!) but remove all other unwanted punctuation.\n",
    "\n",
    "- <b>Handling sentence-ending punctuation:</b>\n",
    "        We’ll replace ? and ! with periods (.) to normalize sentence ends, which is useful for consistent sentence boundaries during training.\n",
    "\n",
    "- <b>Removing chapter headings and titles:</b>\n",
    "        Chapter headings like \"CHAPTER I\" and other titles at the beginning of chapters will be removed to prevent the model from learning irrelevant text structures.\n",
    "\n",
    "- <b>Converting text to lowercase:</b>\n",
    "        By converting everything to lowercase, we ensure that words like \"Invisible\" and \"invisible\" are treated as the same word during training.\n",
    "\n",
    "- <b>Removing extra whitespace:</b>\n",
    "        We’ll also clean up any extra spaces or line breaks in the text so it’s uniformly formatted before being tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a3a2f7-fa25-4a19-879e-be9932201632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "Total number of sentences: 8000\n",
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading txt file...\")\n",
    "with open(r'ponniyin-selvan.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocessing: Replacing punctuation and cleaning\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_ \")\n",
    "text = text.replace(\":\", \" _comma_ \")\n",
    "text = text.replace(\";\", \" _comma_ \")\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace('\"', \"\")  # Remove double quotes\n",
    "text = text.replace(\"'\", \"\")  # Remove single quotes\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"\\u200c\", \"\")  # Remove zero-width non-joiner\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "\n",
    "# Additional cleaning\n",
    "text = clean_numbers(text)\n",
    "text = clean_roman_numerals(text)\n",
    "\n",
    "# Sentence splitting using indic-nlp-library for Tamil\n",
    "sentences = sentence_split(text, lang='ta')  # Tamil language code\n",
    "\n",
    "# Lowercase and tokenize the sentences\n",
    "sentences = [s.lower().strip() for s in sentences if len(s.split()) > 2]\n",
    "tokenized_sentences = [indic_tokenize.trivial_tokenize(s, lang='ta') for s in sentences]\n",
    "\n",
    "# Now, limit the corpus to the first 6,000 sentences\n",
    "num_sentences_to_use = 8000\n",
    "tokenized_sentences = tokenized_sentences[:num_sentences_to_use]\n",
    "\n",
    "print(f\"Total number of sentences: {len(tokenized_sentences)}\")\n",
    "\n",
    "print(\"Preprocessing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc295271-741c-4966-82a3-0df78879732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['பொன்னியின் செல்வன் வரலாற்றுப் புதினம் அமரர் கல்கி கிருஷ்ணமூர்த்தி அத்தியாயம்   ஆடித்திருநாள் ஆதி அந்தமில்லாத கால வெள்ளத்தில் கற்பனை ஓடத்தில் ஏறி நம்முடன் சிறிது நேரம் பிரயாணம் செய்யுமாறு நேயர்களை அழைக்கிறோம்.',\n",
       " 'விநாடிக்கு ஒரு நூற்றாண்டூ வீதம் எளிதில் கடந்து இன்றைக்குத் தொள்ளாயிரத்து எண்பத்திரண்டூ (ல் எழுதியது) ஆண்டூகளுக்கு முந்திய காலத்துக்குச் செல்வோமாக.',\n",
       " 'தொண்டை நாட்டுக்கும் சோழ நாட்டுக்கும் இடையில் உள்ள திருமுனைப்பாடி நாட்டின் தென்பகுதியில் _comma_ தில்லைச் சிற்றம்பலத்துக்கு மேற்கே இரண்டூ காததூரத்தில் _comma_ அலை கடல் போன்ற ஓர் ஏரி விரிந்து பரந்து கிடக்கிறது.',\n",
       " 'அதற்கு வீரநாராயண ஏரி என்று பெயர்.',\n",
       " 'அது தெற்கு வடக்கில் ஒன்றரைக் காத நீளமும் கிழக்கு மேற்கில் அரைக் காத அகலமும் உள்ளது.',\n",
       " 'காலப்போக்கில் அதன் பெயர் சிதைந்து இந்நாளில் வீராணத்து ஏரி என்ற பெயரால் வழங்கி வருகிறது.',\n",
       " 'புது வெள்ளம் வந்து பாய்ந்து ஏரியில் நீர் நிரம்பித் ததும்பி நிற்கும் ஆடி ஆவணி மாதங்களில் வீரநாராயண ஏரியைப் பார்ப்பவர் எவரும் நம்முடைய பழந்தமிழ் நாட்டு முன்னோர்கள் தங்கள் காலத்தில் சாதித்த அரும்பெரும் காரியங்களைக் குறித்துப் பெருமிதமும் பெரு வியப்பும் கொள்ளாமலிருக்க முடியாது.',\n",
       " 'நம் மூதாதையர்கள் தங்களுடைய நலனுக்கும் தங்கள் காலத்திய மக்களின் நலனுக்கும் உரிய காரியங்களை மட்டூமா செய்தார்கள்.',\n",
       " 'தாய்த் திருநாட்டில் தங்களுக்குப் பிற்காலத்தில் வாழையடி வாழையாக வரப்போகும் ஆயிரங்கால சந்ததிகளுக்கும் நன்மை பயக்கும் மாபெரும் செயல்களை நிறைவேற்றி விட்டுப் போனார்கள் அல்லவா.',\n",
       " 'ஆடித் திங்கள் பதினெட்டாம் நாள் முன் மாலை நேரத்தில் அலை கடல் போல் விரிந்து பரந்திருந்த வீர நாராயண ஏரிக்கரை மீது ஒரு வாலிப வீரன் குதிரை ஏறிப் பிரயாணம் செய்து கொண்டிருந்தான்.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9310ddf",
   "metadata": {},
   "source": [
    "We are going to tokenize the text and add start and end tokens:\n",
    "\n",
    "- <b>Tokenize the text</b>:\n",
    "        We’ll split the text into sentences using tokenize.sent_tokenize() and count the total number of sentences.\n",
    "\n",
    "- <b>Add start and end tokens:</b>\n",
    "        For each sentence, we’ll add SENTENCE_START at the beginning and SENTENCE_END at the end to help the model understand sentence boundaries.\n",
    "\n",
    "- <b>Example output:</b>\n",
    "        We’ll print the first 10 tokenized sentences to verify that everything is working as expected.\n",
    "\n",
    "\n",
    "Now we are going to clean and tokenize the sentences:\n",
    "\n",
    "- <b>Remove unwanted punctuation:</b>\n",
    "        We’ll remove periods from the tokenized sentences while keeping the sentence boundaries intact. This ensures we don’t lose important punctuation like SENTENCE_START and SENTENCE_END.\n",
    "- <b>Count word frequencies:</b>\n",
    "        Using Counter, we’ll count how often each word appears in the text and print the total number of unique word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ad2aca-b19e-4450-8105-ba14493e8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21129 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenize and build vocabulary\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Add SENTENCE_START and SENTENCE_END tokens\n",
    "tokenized_sentences = [[sentence_start_token] + sentence + [sentence_end_token] for sentence in tokenized_sentences]\n",
    "# Flatten tokenized sentences to get all words\n",
    "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"Found {len(word_freq)} unique word tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed06bd5-59a3-4e51-a39d-05ff90ca95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected vocabulary size: 16137 with 95.0% coverage\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary to cover 95% of the text\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "total_word_count = sum(word_freq.values())\n",
    "coverage = 0\n",
    "vocab_size = 0\n",
    "desired_coverage = 0.95\n",
    "for word, count in word_freq.most_common():\n",
    "    coverage += count / total_word_count\n",
    "    vocab_size += 1\n",
    "    if coverage >= desired_coverage:\n",
    "        break\n",
    "\n",
    "print(f\"Selected vocabulary size: {vocab_size} with {desired_coverage * 100}% coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d57752",
   "metadata": {},
   "source": [
    "1. We sort the words by frequency and select the most common ones to cover 95% of the total word occurrences, determining the vocabulary size.  \n",
    "2. Mappings (`index_to_word` and `word_to_index`) are created for the vocabulary, including an `unknown_token` for out-of-vocabulary words.  \n",
    "3. Rare words in the tokenized sentences are replaced with `UNKNOWN_TOKEN` to ensure consistency during training.  \n",
    "4. An example sentence is shown with rare words replaced to verify how sentences look after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360a9be6-ebfa-430a-b0df-d936f702454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence after replacing rare words: ['SENTENCE_START', 'பொன்னியின்', 'செல்வன்', 'வரலாற்றுப்', 'புதினம்', 'அமரர்', 'கல்கி', 'கிருஷ்ணமூர்த்தி', 'அத்தியாயம்', 'ஆடித்திருநாள்', 'ஆதி', 'அந்தமில்லாத', 'கால', 'வெள்ளத்தில்', 'கற்பனை', 'ஓடத்தில்', 'ஏறி', 'நம்முடன்', 'சிறிது', 'நேரம்', 'பிரயாணம்', 'செய்யுமாறு', 'நேயர்களை', 'அழைக்கிறோம்', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# Create mappings from word to index and index to word\n",
    "vocab = word_freq.most_common(vocab_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = {word: i for i, word in enumerate(index_to_word)}\n",
    "\n",
    "# Replace words not in the vocabulary with UNKNOWN_TOKEN\n",
    "tokenized_sentences = [[word if word in word_to_index else unknown_token for word in sentence] for sentence in tokenized_sentences]\n",
    "tokenized_sentences = [\n",
    "    [word for word in sentence if word != '.']\n",
    "    for sentence in tokenized_sentences\n",
    "]\n",
    "# Show an example sentence after rare word handling\n",
    "print(f\"Example sentence after replacing rare words: {tokenized_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8281d044-40e6-43a8-89cb-e92f82ec173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentence: ['SENTENCE_START', 'இதைக்', 'குறித்து', 'நாடெங்கும்', 'கொஞ்சம்', 'பரிகாசப்', 'பேச்சு', 'நடந்து', 'வருகிறது', 'வந்தியத்தேவா', 'ஒரு', 'பிராயத்தைத்', 'தாண்டியவர்களுக்கு', 'இந்த', 'மாதிரி', 'ஸ்திரீ', 'சபலம்', 'ஏற்பட்டால்', 'எல்லோருக்கும்', 'சிறிது', 'இளக்காரமாகத்தானே', 'இருக்கும்', 'SENTENCE_END']\n",
      "Sentence as indices: [1, 447, 1052, 1589, 124, 2310, 579, 100, 337, 3149, 7, 8337, 8338, 18, 389, 2376, 4591, 4592, 1962, 36, 8339, 220, 2]\n",
      "Sentence from indices: ['SENTENCE_START', 'இதைக்', 'குறித்து', 'நாடெங்கும்', 'கொஞ்சம்', 'பரிகாசப்', 'பேச்சு', 'நடந்து', 'வருகிறது', 'வந்தியத்தேவா', 'ஒரு', 'பிராயத்தைத்', 'தாண்டியவர்களுக்கு', 'இந்த', 'மாதிரி', 'ஸ்திரீ', 'சபலம்', 'ஏற்பட்டால்', 'எல்லோருக்கும்', 'சிறிது', 'இளக்காரமாகத்தானே', 'இருக்கும்', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select a random sentence from tokenized_sentences\n",
    "random_sentence = random.choice(tokenized_sentences)\n",
    "\n",
    "# Convert the sentence to indices using word_to_index\n",
    "sentence_indices = [word_to_index[word] for word in random_sentence]\n",
    "\n",
    "# Convert the indices back to words using index_to_word\n",
    "sentence_words = [index_to_word[index] for index in sentence_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Random sentence:\", random_sentence)\n",
    "print(\"Sentence as indices:\", sentence_indices)\n",
    "print(\"Sentence from indices:\", sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5389f",
   "metadata": {},
   "source": [
    "## Step 3: Generating N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f48c1",
   "metadata": {},
   "source": [
    "Range of n-grams: We are harvesting all n-grams from bigrams (2-grams) to 20-grams by iterating through lengths from 2 to 20.\n",
    "\n",
    "Count n-grams: For each length i, the function ngrams() is used to generate all possible n-grams of that length from the text. The Counter is then used to count the occurrences of each n-gram.\n",
    "\n",
    "Store n-grams: The counts for each n-gram length are printed and stored in the ngrams_up_to_20 list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcdd4e6-eb11-4923-ae41-9cfe374e3f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram-2 length: 326762\n",
      "ngram-3 length: 416139\n",
      "ngram-4 length: 427866\n",
      "ngram-5 length: 429198\n",
      "ngram-6 length: 429397\n",
      "ngram-7 length: 429440\n",
      "ngram-8 length: 429454\n",
      "ngram-9 length: 429456\n",
      "ngram-10 length: 429457\n",
      "ngram-11 length: 429457\n",
      "ngram-12 length: 429456\n",
      "ngram-13 length: 429455\n",
      "ngram-14 length: 429454\n",
      "ngram-15 length: 429453\n",
      "ngram-16 length: 429452\n",
      "ngram-17 length: 429451\n",
      "ngram-18 length: 429450\n",
      "ngram-19 length: 429449\n",
      "ngram-20 length: 429448\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Harvesting all n-grams up to length 20\n",
    "ngrams_up_to_20 = []\n",
    "for i in range(2, 21):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))  # Collecting n-grams of length i\n",
    "    print(f'ngram-{i} length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6efeb7",
   "metadata": {},
   "source": [
    "We need to ensure that the n-grams we keep are complete and not broken by sentence-ending punctuation. So, we will implement helper functions to enure that. They are:\n",
    "- remove_periods(): This function checks if any word in the n-gram contains a period or quotation mark. If any such characters are found, the function returns False, indicating that the n-gram should be excluded.\n",
    "\n",
    "- my_filter(): This function applies remove_periods() to a list of n-grams, filtering out any n-grams that span sentence boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b275f1-9f28-4ed9-b48a-55de10392a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove n-grams that contain periods or quotes\n",
    "def remove_periods(ngram):\n",
    "    \"\"\"Remove n-grams that contain periods or quotes.\"\"\"\n",
    "    for word in ngram[0]:\n",
    "        if '.' in word or '’' in word or '‘' in word:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Keep only repeating n-grams\n",
    "def my_filter(ngrams):\n",
    "    \"\"\"Filter n-grams to only keep those that occur more than once and do not span sentence boundaries.\"\"\"\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca313e",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d211046",
   "metadata": {},
   "source": [
    "Now, we'll construct the training dataset (X_train and y_train) using n-grams from 2-grams to 20-grams:\n",
    "\n",
    "- Initialize training data: Empty lists X_train and y_train are created to store the input sequences and target words.\n",
    "\n",
    "- Process n-grams: For each set of n-grams (from bigrams to 20-grams), we iterate through the most common n-grams.\n",
    "\n",
    "- Filter valid n-grams: Using my_filter(), we ensure all n-grams pass certain conditions, and only those where all words are in the vocabulary (word_to_index) are considered.\n",
    "\n",
    "- Create training examples:\n",
    "        X_train: The input sequence consists of the n-gram minus the last word.\n",
    "        y_train: The target is the last word of the n-gram.\n",
    "\n",
    "- Final count: The total number of sequences generated from n-grams is printed, showing how many training examples were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532fcee3-dc59-4348-a6ac-e239de8f4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences from n-grams: 340256\n"
     ]
    }
   ],
   "source": [
    "# Initialize training data lists\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process all n-grams from 2 to 20\n",
    "for i in range(len(ngrams_up_to_20)):  # Starting from bigrams\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "\n",
    "    # Construct X_train and y_train using the filtered n-grams\n",
    "    for sent in my_filter(ngrams_to_learn.most_common()):\n",
    "        ngram = sent[0]\n",
    "        # Ensure all words are in vocabulary\n",
    "        if all(word in word_to_index for word in ngram):\n",
    "            ngram_indices = [word_to_index[word] for word in ngram]\n",
    "            # Input sequence is the n-gram minus the last word\n",
    "            X_train.append(ngram_indices[:-1])\n",
    "            # Target is the last word of the n-gram\n",
    "            y_train.append(ngram_indices[-1])\n",
    "\n",
    "print(f'Total sequences from n-grams: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf8890",
   "metadata": {},
   "source": [
    "We now expand the training dataset by incorporating sequences from complete tokenized sentences. For each sentence, we ensure that all words are in the vocabulary, and then we create input sequences using the words leading up to the current word, with the current word as the target output. This allows the model to learn from entire sentence structures, improving its ability to predict the next word based on the broader context of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc62717-31f6-4fe9-847c-2ffe32bb5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences after including sentences: 421996\n"
     ]
    }
   ],
   "source": [
    "# Include sequences from your tokenized sentences\n",
    "for sentence in tokenized_sentences:\n",
    "    if all(word in word_to_index for word in sentence):\n",
    "        sentence_indices = [word_to_index[word] for word in sentence]\n",
    "        for i in range(1, len(sentence_indices)):\n",
    "            X_train.append(sentence_indices[:i])\n",
    "            y_train.append(sentence_indices[i])\n",
    "\n",
    "print(f'Total sequences after including sentences: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f32a6",
   "metadata": {},
   "source": [
    "We combine X_train (input sequences) and y_train (target words) into a single list of tuples, then shuffle them together using random.shuffle().\n",
    "After shuffling, we unpack the combined list back into X_train and y_train, keeping the pairs aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d42a0a1-a1ff-44d5-af6b-d490581c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "combined = list(zip(X_train, y_train))\n",
    "random.shuffle(combined)\n",
    "X_train[:], y_train[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae31a163",
   "metadata": {},
   "source": [
    "we are preparing the training data by padding the sequences and converting them into the appropriate format for model training:\n",
    "\n",
    "- Determine maximum sequence length: We calculate the length of the longest sequence in X_train to use this as the standard for padding all sequences to the same length.\n",
    "\n",
    "- Pad sequences: Using pad_sequences(), we pad the input sequences (X_train) with zeros at the beginning (pre-padding). This ensures that all sequences have the same length, which is necessary for training models that expect fixed-length input.\n",
    "\n",
    "- Convert y_train to a NumPy array: We convert y_train to a NumPy array, making it compatible with machine learning libraries that require data in this format.\n",
    "\n",
    "- Print shape of padded data: We print the shape of the padded X_train and y_train to verify that they are ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d592d56-b426-4105-afc9-7baecace42a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 75\n",
      "X_train_padded shape: (421996, 75), y_train shape: (421996,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in X_train)\n",
    "print(f'Max sequence length: {max_seq_length}')\n",
    "\n",
    "# Pad sequences with zeros at the beginning\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# Convert y_train to a NumPy array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f'X_train_padded shape: {X_train_padded.shape}, y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f40bb",
   "metadata": {},
   "source": [
    "We are splitting the dataset into training and validation sets. Using train_test_split(), 90% of the data is allocated for training, while 10% is reserved for validation. The validation set helps us assess the model's performance on unseen data, ensuring it generalizes well beyond the training set. The split is made reproducible by setting a random seed (random_state=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "185ed9ee-9f55-45da-8bf2-b0e4e355983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (379796, 75), (379796,)\n",
      "Validation data shape: (42200, 75), (42200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train_padded_train, X_train_padded_val, y_train_train, y_train_val = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training data shape: {X_train_padded_train.shape}, {y_train_train.shape}')\n",
    "print(f'Validation data shape: {X_train_padded_val.shape}, {y_train_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef36de",
   "metadata": {},
   "source": [
    "## Step 5: Saving all the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe76528",
   "metadata": {},
   "source": [
    "Since this is a model that takes extensive tuning and training, we are saving the processed training data and vocabulary to disk using pickle.\n",
    "\n",
    "The main idea behind pickling is to avoid having to redo the entire preprocessing each time we want to use the data. By saving X_train, y_train, the tokenized sentences, and the word-to-index and index-to-word mappings, we can easily reload them later. \n",
    "\n",
    "This makes sure that we always have access to all the necessary data for training, predictions and further analysis, as it can be loaded quickly without re-running the entire data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3be1854-89dd-4a3e-9809-42aace6296b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the processed data\n",
    "with open('pickle/X_train_padded_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_padded, file)\n",
    "with open('pickle/y_train_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "with open('pickle/tokenized_sentences_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenized_sentences, file)\n",
    "with open('pickle/word_to_index_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(word_to_index, file)\n",
    "with open('pickle/index_to_word_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(index_to_word, file)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b46d78-10be-4aad-a204-48016aae65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CBOW embeddings from your saved file\n",
    "# embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# # Open your CBOW embedding file\n",
    "# with open('my_cbow_vectors_ps.txt', 'r', encoding='utf-8') as f:\n",
    "#     # Read the header\n",
    "#     vocab_size, embedding_dim = map(int, f.readline().split())\n",
    "    \n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]  # The word itself\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "#         embeddings_index[word] = coefs\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Create the embedding matrix for the CBOW embeddings\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# for word, i in word_to_index.items():\n",
    "#     if i >= vocab_size:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669fea16-54d1-4d32-b20f-a5c0457b7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GloVe embeddings\n",
    "# embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# # Open the GloVe embeddings file\n",
    "# with open('my_cbow_vectors_ps.txt', encoding='utf8') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]  # The word itself\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "#         embeddings_index[word] = coefs\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Create the embedding matrix\n",
    "# embedding_dim = 100\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# print(vocab_size)\n",
    "# for word, i in word_to_index.items():\n",
    "#     if i >= vocab_size:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0105a64d",
   "metadata": {},
   "source": [
    "## Step 6: Building our Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8ee499",
   "metadata": {},
   "source": [
    "We define an RNN model using Keras with two LSTM layers for text sequence prediction:\n",
    "- Embedding layer: We use creating embeddings in our embedding layer, by using the input data that is trainable.\n",
    "- LSTM layers: We add two LSTM layers with 64 hidden units each, followed by Dropout and BatchNormalization to prevent overfitting and stabilize training.\n",
    "- Output layer: We include a Dense layer with softmax activation to predict the next word.\n",
    "- ReduceLROnPlateau: We implement ReduceLROnPlateau to adjust the learning rate when the loss plateaus, improving convergence.\n",
    "- Compilation: We compile the model with the Adam optimizer and sparse_categorical_crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b89774-db77-47d8-a799-abf5e6c59c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 75, 100)           1613700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75, 64)            42240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_norm_1 (TimeDistribute (None, 75, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_norm_2 (BatchNormaliza (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 16137)             1048905   \n",
      "=================================================================\n",
      "Total params: 2,738,381\n",
      "Trainable params: 2,738,125\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Hidden dimensions for LSTM layers\n",
    "hidden_dim = 64\n",
    "\n",
    "# Input Layer\n",
    "inputs = Input(shape=(max_seq_length,), name='input_layer')\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=100,\n",
    "    name='embedding_layer',\n",
    "    trainable=True\n",
    ")(inputs)\n",
    "\n",
    "# First LSTM Layer\n",
    "lstm1 = LSTM(units=hidden_dim, return_sequences=True, name='lstm_1')(embedding)\n",
    "dropout1 = Dropout(0.1, name='dropout_1')(lstm1)\n",
    "\n",
    "# BatchNormalization Wrapped with TimeDistributed\n",
    "bn1 = TimeDistributed(BatchNormalization(), name='batch_norm_1')(dropout1)\n",
    "\n",
    "# Second LSTM Layer\n",
    "lstm2 = LSTM(units=hidden_dim, name='lstm_2')(bn1)\n",
    "dropout2 = Dropout(0.1, name='dropout_2')(lstm2)\n",
    "bn2 = BatchNormalization(name='batch_norm_2')(dropout2)\n",
    "\n",
    "# Output Layer\n",
    "outputs = Dense(vocab_size, activation='softmax', name='output_layer')(bn2)\n",
    "\n",
    "# Define the Model\n",
    "model = Model(inputs=inputs, outputs=outputs, name='rnn_model')\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Define the learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c8c17",
   "metadata": {},
   "source": [
    "## Step 7: Training our RNN Model on Ponniyn Selvan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31544657",
   "metadata": {},
   "source": [
    "We are training the model using the fit method, where the training data (X_train_padded_train and y_train_train) is used to adjust the model's weights over 150 epochs, with a batch size of 128. The validation data (X_train_padded_val and y_train_val) is used to monitor the model's performance on unseen data during training. The ReduceLROnPlateau callback reduces the learning rate if the loss stops improving, helping the model converge more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d41ba32-292e-4f37-ba05-9f62f1c3f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokes\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379796 samples, validate on 42200 samples\n",
      "Epoch 1/50\n",
      "379796/379796 [==============================] - 678s 2ms/step - loss: 7.4762 - val_loss: 6.2187\n",
      "Epoch 2/50\n",
      "379796/379796 [==============================] - 705s 2ms/step - loss: 5.5065 - val_loss: 5.0085\n",
      "Epoch 3/50\n",
      "379796/379796 [==============================] - 691s 2ms/step - loss: 4.5651 - val_loss: 4.5413\n",
      "Epoch 4/50\n",
      "379796/379796 [==============================] - 689s 2ms/step - loss: 4.1196 - val_loss: 4.3076\n",
      "Epoch 5/50\n",
      "379796/379796 [==============================] - 679s 2ms/step - loss: 3.8549 - val_loss: 4.1625\n",
      "Epoch 6/50\n",
      "379796/379796 [==============================] - 683s 2ms/step - loss: 3.6800 - val_loss: 4.0728\n",
      "Epoch 7/50\n",
      "379796/379796 [==============================] - 672s 2ms/step - loss: 3.5454 - val_loss: 4.0099\n",
      "Epoch 8/50\n",
      "379796/379796 [==============================] - 674s 2ms/step - loss: 3.4441 - val_loss: 3.9601\n",
      "Epoch 9/50\n",
      "379796/379796 [==============================] - 682s 2ms/step - loss: 3.3540 - val_loss: 3.9051\n",
      "Epoch 10/50\n",
      "379796/379796 [==============================] - 678s 2ms/step - loss: 3.2798 - val_loss: 3.8734\n",
      "Epoch 11/50\n",
      "379796/379796 [==============================] - 683s 2ms/step - loss: 3.2224 - val_loss: 3.8434\n",
      "Epoch 12/50\n",
      "379796/379796 [==============================] - 683s 2ms/step - loss: 3.1638 - val_loss: 3.8189\n",
      "Epoch 13/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 3.1131 - val_loss: 3.8071\n",
      "Epoch 14/50\n",
      "379796/379796 [==============================] - 683s 2ms/step - loss: 3.0657 - val_loss: 3.7789\n",
      "Epoch 15/50\n",
      "379796/379796 [==============================] - 686s 2ms/step - loss: 3.0272 - val_loss: 3.7820\n",
      "Epoch 16/50\n",
      "379796/379796 [==============================] - 693s 2ms/step - loss: 2.9899 - val_loss: 3.7536\n",
      "Epoch 17/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 2.9515 - val_loss: 3.7405\n",
      "Epoch 18/50\n",
      "379796/379796 [==============================] - 703s 2ms/step - loss: 2.9220 - val_loss: 3.7460\n",
      "Epoch 19/50\n",
      "379796/379796 [==============================] - 696s 2ms/step - loss: 2.8933 - val_loss: 3.7365\n",
      "Epoch 20/50\n",
      "379796/379796 [==============================] - 697s 2ms/step - loss: 2.8649 - val_loss: 3.7195\n",
      "Epoch 21/50\n",
      "379796/379796 [==============================] - 702s 2ms/step - loss: 2.8383 - val_loss: 3.7167\n",
      "Epoch 22/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 2.8174 - val_loss: 3.7062\n",
      "Epoch 23/50\n",
      "379796/379796 [==============================] - 701s 2ms/step - loss: 2.7923 - val_loss: 3.7050\n",
      "Epoch 24/50\n",
      "379796/379796 [==============================] - 689s 2ms/step - loss: 2.7723 - val_loss: 3.7129\n",
      "Epoch 25/50\n",
      "379796/379796 [==============================] - 700s 2ms/step - loss: 2.7529 - val_loss: 3.7209\n",
      "Epoch 26/50\n",
      "379796/379796 [==============================] - 695s 2ms/step - loss: 2.7365 - val_loss: 3.7101\n",
      "Epoch 27/50\n",
      "379796/379796 [==============================] - 687s 2ms/step - loss: 2.7196 - val_loss: 3.6999\n",
      "Epoch 28/50\n",
      "379796/379796 [==============================] - 693s 2ms/step - loss: 2.7036 - val_loss: 3.6945\n",
      "Epoch 29/50\n",
      "379796/379796 [==============================] - 695s 2ms/step - loss: 2.6896 - val_loss: 3.6935\n",
      "Epoch 30/50\n",
      "379796/379796 [==============================] - 695s 2ms/step - loss: 2.6741 - val_loss: 3.7005\n",
      "Epoch 31/50\n",
      "379796/379796 [==============================] - 700s 2ms/step - loss: 2.6603 - val_loss: 3.6957\n",
      "Epoch 32/50\n",
      "379796/379796 [==============================] - 695s 2ms/step - loss: 2.6458 - val_loss: 3.7197\n",
      "Epoch 33/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 2.6357 - val_loss: 3.7014\n",
      "Epoch 34/50\n",
      "379796/379796 [==============================] - 698s 2ms/step - loss: 2.6258 - val_loss: 3.7078\n",
      "Epoch 35/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 2.6127 - val_loss: 3.6986\n",
      "Epoch 36/50\n",
      "379796/379796 [==============================] - 698s 2ms/step - loss: 2.6021 - val_loss: 3.7372\n",
      "Epoch 37/50\n",
      "379796/379796 [==============================] - 693s 2ms/step - loss: 2.5940 - val_loss: 3.6948\n",
      "Epoch 38/50\n",
      "379796/379796 [==============================] - 696s 2ms/step - loss: 2.5841 - val_loss: 3.7154\n",
      "Epoch 39/50\n",
      "379796/379796 [==============================] - 701s 2ms/step - loss: 2.5747 - val_loss: 3.7103\n",
      "Epoch 40/50\n",
      "379796/379796 [==============================] - 709s 2ms/step - loss: 2.5697 - val_loss: 3.7183\n",
      "Epoch 41/50\n",
      "379796/379796 [==============================] - 696s 2ms/step - loss: 2.5564 - val_loss: 3.7228\n",
      "Epoch 42/50\n",
      "379796/379796 [==============================] - 694s 2ms/step - loss: 2.5527 - val_loss: 3.7221\n",
      "Epoch 43/50\n",
      "379796/379796 [==============================] - 697s 2ms/step - loss: 2.5447 - val_loss: 3.7120\n",
      "Epoch 44/50\n",
      "379796/379796 [==============================] - 708s 2ms/step - loss: 2.5342 - val_loss: 3.7251\n",
      "Epoch 45/50\n",
      "379796/379796 [==============================] - 701s 2ms/step - loss: 2.5269 - val_loss: 3.7111\n",
      "Epoch 46/50\n",
      "379796/379796 [==============================] - 706s 2ms/step - loss: 2.5202 - val_loss: 3.7080\n",
      "Epoch 47/50\n",
      "379796/379796 [==============================] - 674s 2ms/step - loss: 2.5139 - val_loss: 3.7179\n",
      "Epoch 48/50\n",
      "379796/379796 [==============================] - 515s 1ms/step - loss: 2.5068 - val_loss: 3.7095\n",
      "Epoch 49/50\n",
      "379796/379796 [==============================] - 515s 1ms/step - loss: 2.5046 - val_loss: 3.7138\n",
      "Epoch 50/50\n",
      "379796/379796 [==============================] - 515s 1ms/step - loss: 2.4969 - val_loss: 3.7308\n"
     ]
    }
   ],
   "source": [
    "# Now retry training the model\n",
    "history = model.fit(\n",
    "    X_train_padded_train, y_train_train,\n",
    "    validation_data=(X_train_padded_val, y_train_val),\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    callbacks=[reduce_lr]  # Include the learning rate callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035304d",
   "metadata": {},
   "source": [
    "## Step 8: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712cdc59-b00e-46e5-85d0-f8c1ecd4611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights using pickle\n",
    "# with open('tensorflow_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "model.save('model/rnn_direct_model.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8588f",
   "metadata": {},
   "source": [
    "## Step 9: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229574b",
   "metadata": {},
   "source": [
    "#### Generate Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6448e",
   "metadata": {},
   "source": [
    "The generate_sentence() function uses the trained RNN model to generate sentences. It starts with the sentence_start_token and predicts each subsequent word by sampling from the model's probability distribution for the next word. The generation continues until it reaches the sentence_end_token or a specified sentence length. The predicted word indices are then converted back into actual words, forming a complete sentence. This function enables the model to create coherent text based on the patterns it has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b363eed4-fcbb-4c56-85cb-f5a1fb180687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, word_to_index, index_to_word, max_seq_length, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the maximum sentence length\n",
    "    while (new_sentence[-1] != word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word, avoiding UNKNOWN_TOKEN\n",
    "        sampled_word_index = word_to_index[unknown_token]\n",
    "        while sampled_word_index == word_to_index[unknown_token]:\n",
    "            # Sample from the distribution\n",
    "            sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Append the sampled word to the sentence\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices to words, excluding SENTENCE_START and SENTENCE_END tokens\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence[1:-1]]\n",
    "    return ' '.join(sentence_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfcfb3",
   "metadata": {},
   "source": [
    "The generate_sentence() function uses the trained RNN model to generate sentences. It starts with the sentence_start_token and predicts each subsequent word by sampling from the model's probability distribution for the next word. The generation continues until it reaches the sentence_end_token or a specified sentence length. The predicted word indices are then converted back into actual words, forming a complete sentence. This function enables the model to create coherent text based on the patterns it has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df767a57-c3a4-4036-8f7b-2f658931a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "” “ஒரு நாளும் கொடுத்து விட்டூ உட்கார்ந்து கொண்டான்\n",
      "அதுவரைக்கும் இங்கே செய்ய வேண்டிய ஏற்பாடுகள் இன்னும் சில கொண்டிருந்தது\n",
      "இவ்விதம் குந்தவைதேவி கேலியாகக் கேட்டாள் “அக்கா குதிரை குதிரை காலடிச் சத்தம் கேட்கிறது\n",
      "நான் வேண்டுமானாலும் உள்ளே வந்தாயே அந்த என் சீடனுக்குள்ளே இருப்பவரும் கடவுள்தான் ” “போதும் _ comma _ போதும் _ comma _\n",
      "அதன் வந்த மாளிகை கிடந்த முதலையின் மேல் பட்டுப் பீதாம்பரங்கள் உனக்குக் கொடுத்திருக்கிறோமே\n",
      "என்னும் பெயரும் எனக்கு எவ்வளவு தெரிந்திருக்க வேண்டியது கேட்டுக் கொண்டே நின்றான்\n",
      "தங்களைத் தவிர எப்படியும் வேறு காஞ்சிக்குப் போகவே வார்த்தை பார்க்க வேண்டும் _ comma _ இந்த சோழ ராஜ்யத்தின் இதுவரை எனக்கே நினைத்துக்\n",
      "இராஜரீக காரியங்களில் நாள் நட்சத்திரம் _ comma _ ஜாதகம் எல்லாம் குமரி\n",
      "தான்தொங்கிக் கலிங்கராயர் கொண்டாட்டங்கள் முதல் காலத்தில் வடக்கே வெல்லாம் _ comma _ மோதினார்கள் என்றா நினைத்தது நந்தினி அங்கேயே நிற்பதைக் கண்டதும் அப்படி\n",
      "“அளந்து பார்த்ததில் இருவரும் சமமான உயரமே மூடமதி என்று நீ சொன்னது சரிதான் ஏனென்றால் _ comma _ உன் உற்றுப் பார்த்ததில் இருளடைந்த\n",
      "அடிக்கடி தெரியும் _ comma _ இந்த கையையும் அனுசிதமான அல்லவா செய்ய வேண்டும்\n",
      "பாண்டிய ஒருவர் ஸ்தலங்களும் மூவரின் பாடல் பெற்ற சிவ ஸ்தலங்களும் புதிய சிறப்பையும் புனிதத் தன்மையையும் அடைந்தன\n",
      "வந்தியத்தேவனுடைய மூளை வேலை ஒரு சிரிப்புச் சத்தம் எழுந்தது\n",
      "ஆனால் ஆண்டாள் பக்தி செய்து _ comma _ வானதி முதலிய அளித்த குலத்து மங்கையாக்க வேண்டுமென்று கொண்டூ வந்து நின்றார் “திருமலை சண்டை\n",
      "” “தாங்கள் என்று கேட்டார் சுந்தர சோழரின் வடக்கு அமைச்சர்களும் “ஓஹோ\n",
      "” “வழியில் என்ன புதல்வனும் போகக் பொய்களை மலர்களைத் தொங்கவிட்டுக் கொண்டு காட்சியளித்தன\n",
      "வாசலுக்கு அப்பால் சிறிது தூரத்தில் தெரிந்த பெண் தான் என்பதை மறந்து விட்டேன்\n",
      "முன் இவள் இப்படிப் தங்களுக்குத் கொண்டன _ comma _ பல கண்டங்கள் ஏற்படும்\n",
      "என்னைத் துவந்த யுத்தம் பற்றிக் எண்ணினார் அவ்வளவு உன் comma _ நாலாம் ஆண்டில் அளித்த பத்து\n",
      "வானதி அடிக்கடி இளம் பெண் குலத்தில் வழிவழியாக வந்த ஏறிய தந்தையாகிய வந்தியத்தேவனுடைய பிரயோகிக்க மாட்டான்\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    sent = ''\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent.split()) < senten_min_length:\n",
    "        sent = generate_sentence(model, word_to_index, index_to_word, max_seq_length, senten_max_length)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc70971",
   "metadata": {},
   "source": [
    "#### Generating based on a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f179",
   "metadata": {},
   "source": [
    "The generate_sentence() function generates a sentence based on a given starting text. It first converts the starting words into word indices using the model's vocabulary. Then, it uses the trained RNN model to predict and append the next word, continuing until the sentence reaches a specified maximum length or the end token is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4735a82-d61b-4751-8e6c-12c7b8e49b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_text, senten_max_length):\n",
    "    # Convert the start_text to lowercase to match your vocabulary\n",
    "    start_text = start_text.lower()\n",
    "    \n",
    "    # Convert the start_text into indices based on your vocabulary\n",
    "    new_sentence = [word_to_index.get(word, word_to_index[unknown_token]) for word in start_text.split()]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the max sentence length\n",
    "    while len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word\n",
    "        sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Check if the sampled word is the SENTENCE_END token\n",
    "        if sampled_word_index == word_to_index.get(sentence_end_token):\n",
    "            break  # Stop adding words if we reach the end token\n",
    "        \n",
    "        # Append the predicted word\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence]\n",
    "    generated_text = ' '.join(sentence_str)\n",
    "    generated_text += '.'\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf65d06",
   "metadata": {},
   "source": [
    "#### <b> Prompts and their Outputs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "184879e0-c9aa-498e-977f-ba257808d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: பொதுவாக ஓர் எண்ணங்கள் வந்தியத்தேவன் இறுக்கிக் கீழே பாய்ந்து ஏறி எதிரே படகு கையில் கொடுக்கும்படி என்னை ஏன் செய்யச் சிவிகையில் ஏற்றி அவரை நாங்கள் சுமந்து செல்லவும் சித்தமாயிருக்கிறோம் அல்லவா உன்னைப் பற்றி ஒன்றும் கவலைப்பட வேண்டாம் நமது கட்டளை உசிதம் என்று சொல்லி அருள வேண்டூம்” என்றான் அமுதன் இருந்த காலத்தில் நாம் என்ன செய்யப் கட்சியில் போட்டி கணங்களைக்கொன்று எடுத்து போட்டுக் கொண்டு திரும்பி வந்து.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"பொதுவாக\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74b86753-13c3-4012-ae10-8828212527d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: வணக்கம் செலுத்திப் பிறகு கண்டதும் மனம் விட்டூச் கேட்க வேண்டும் என்று வந்தியத்தேவனுடைய மனதில் எவ்விதச் சந்தேகமும் சந்தனம் அணிந்து தலையில் முன் குடுமி வைத்திருந்த வைஷ்ணவ பக்த சிரோமணி ஆகப் போவதாகச் சொல்லிக் கொண்டும் இந்தச் கோட்டையில் அவன் சோதிடம் கேட்க இந்தக் கிழவர் இந்த யாருக்குப் பெரிதும் ஆச்சரியம் இனி அல்லவா பழையாறையில் புலவர்களும் கவிஞர்களும் என்னைப் பார்க்க வந்து விட்டார் என்று சொன்னாய் என்று உங்களுக்குத்.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"வணக்கம்\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6acfcf06-f6ad-4536-b3c1-0a2621706b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: சோழன் தன் வாழ்நாளில் தங்களுக்கு மகிழ்ச்சி அடைந்து குதூகலம் வீசிய அந்த வாளை எடுக்க வேண்டும் என்று எனக்கே வியப்பை அளிக்கவில்லை ஒருவாறு தெரிந்து விட்டன என்று நன்றாகத் தெரிந்து அடிக்கடி என் வாழ்நாளில் நான் இந்த திரை ஒரு முகம் அவ்வளவாக அவனுக்குச் அளித்து என் உள்ளத்தில் பொங்கிப் பண்டக சாலைகள் அவரைச் தேடிக் கொண்டிருந்தது.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"சோழன்\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5f3f6bc-ef7d-451d-a2db-70180d79dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights using pickle\n",
    "# with open('tensorflow_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "model.save_weights('model/rnn_direct_model_weight.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613bac4a-04c3-4156-bd7b-7e6271f2460b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
