{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eed3cfc",
   "metadata": {},
   "source": [
    "# 2. Ponniyn Selvan RNN Chatbot with pre-trained CBoW embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd270f",
   "metadata": {},
   "source": [
    "This notebook, the second in this week's assignment, aims to build our first model. Here, we will train the model using pre-trained embeddings from the historic Tamil text Ponniyin Selvan. This setup will later allow us to compare it to a second model, where our RNN will learn the embeddings on its own, which we will construct in the third notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf1e26",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfb2f77-75fc-4958-8b48-d03e8947dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lokes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# For Keras model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "from indicnlp.tokenize.sentence_tokenize import sentence_split\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b823d",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff6606",
   "metadata": {},
   "source": [
    "We are going to set up tokens first:\n",
    "\n",
    "- unknown_token: This token will represent words that aren’t in our vocabulary. It helps the model manage unfamiliar words during training and generation.\n",
    "\n",
    "- sentence_start_token: This token will be added to the beginning of every sentence, so the model understands where sentences start.\n",
    "\n",
    "- sentence_end_token: This token will be placed at the end of each sentence, allowing the model to know when the sentence is complete.\n",
    "\n",
    "These tokens will help the model structure sentences and deal with unknown words effectively during training and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ac42e4-4613-4fde-937d-d724609d419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d5c73",
   "metadata": {},
   "source": [
    "We want to clean any numbers and roman numerals that might arise in the in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ee8742-6467-457b-a7b2-cbd59d7497d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b1154",
   "metadata": {},
   "source": [
    "Next, we are going to read and clean the text from The Invisible Man by H.G. Wells. The goal is to prepare the text so it can be fed into our model without unnecessary punctuation, chapter headings, or formatting issues. Here's how we'll do that:\n",
    "\n",
    "- <b>Reading the file:</b> We’ll open the text file (invisible_man_gutenberg.txt) and read its content into memory.\n",
    "\n",
    "- <b>Removing unwanted punctuation:</b>\n",
    "        We'll remove specific punctuation marks, such as commas, colons, quotes, and dashes, by creating a translation table.\n",
    "        We’ll also use regular expressions to keep sentence-ending punctuation like periods (.), question marks (?), and exclamation marks (!) but remove all other unwanted punctuation.\n",
    "\n",
    "- <b>Handling sentence-ending punctuation:</b>\n",
    "        We’ll replace ? and ! with periods (.) to normalize sentence ends, which is useful for consistent sentence boundaries during training.\n",
    "\n",
    "- <b>Removing chapter headings and titles:</b>\n",
    "        Chapter headings like \"CHAPTER I\" and other titles at the beginning of chapters will be removed to prevent the model from learning irrelevant text structures.\n",
    "\n",
    "- <b>Converting text to lowercase:</b>\n",
    "        By converting everything to lowercase, we ensure that words like \"Invisible\" and \"invisible\" are treated as the same word during training.\n",
    "\n",
    "- <b>Removing extra whitespace:</b>\n",
    "        We’ll also clean up any extra spaces or line breaks in the text so it’s uniformly formatted before being tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a3a2f7-fa25-4a19-879e-be9932201632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "Total number of sentences: 8000\n",
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading txt file...\")\n",
    "with open(r'ponniyin-selvan.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocessing: Replacing punctuation and cleaning\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_ \")\n",
    "text = text.replace(\":\", \" _comma_ \")\n",
    "text = text.replace(\";\", \" _comma_ \")\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace('\"', \"\")  # Remove double quotes\n",
    "text = text.replace(\"'\", \"\")  # Remove single quotes\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"\\u200c\", \"\")  # Remove zero-width non-joiner\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "\n",
    "# Additional cleaning\n",
    "text = clean_numbers(text)\n",
    "text = clean_roman_numerals(text)\n",
    "\n",
    "# Sentence splitting using indic-nlp-library for Tamil\n",
    "sentences = sentence_split(text, lang='ta')  # Tamil language code\n",
    "\n",
    "# Lowercase and tokenize the sentences\n",
    "sentences = [s.lower().strip() for s in sentences if len(s.split()) > 2]\n",
    "tokenized_sentences = [indic_tokenize.trivial_tokenize(s, lang='ta') for s in sentences]\n",
    "\n",
    "# Now, limit the corpus to the first 6,000 sentences\n",
    "num_sentences_to_use = 8000\n",
    "tokenized_sentences = tokenized_sentences[:num_sentences_to_use]\n",
    "\n",
    "print(f\"Total number of sentences: {len(tokenized_sentences)}\")\n",
    "\n",
    "print(\"Preprocessing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc295271-741c-4966-82a3-0df78879732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['பொன்னியின் செல்வன் வரலாற்றுப் புதினம் அமரர் கல்கி கிருஷ்ணமூர்த்தி அத்தியாயம்   ஆடித்திருநாள் ஆதி அந்தமில்லாத கால வெள்ளத்தில் கற்பனை ஓடத்தில் ஏறி நம்முடன் சிறிது நேரம் பிரயாணம் செய்யுமாறு நேயர்களை அழைக்கிறோம்.',\n",
       " 'விநாடிக்கு ஒரு நூற்றாண்டூ வீதம் எளிதில் கடந்து இன்றைக்குத் தொள்ளாயிரத்து எண்பத்திரண்டூ (ல் எழுதியது) ஆண்டூகளுக்கு முந்திய காலத்துக்குச் செல்வோமாக.',\n",
       " 'தொண்டை நாட்டுக்கும் சோழ நாட்டுக்கும் இடையில் உள்ள திருமுனைப்பாடி நாட்டின் தென்பகுதியில் _comma_ தில்லைச் சிற்றம்பலத்துக்கு மேற்கே இரண்டூ காததூரத்தில் _comma_ அலை கடல் போன்ற ஓர் ஏரி விரிந்து பரந்து கிடக்கிறது.',\n",
       " 'அதற்கு வீரநாராயண ஏரி என்று பெயர்.',\n",
       " 'அது தெற்கு வடக்கில் ஒன்றரைக் காத நீளமும் கிழக்கு மேற்கில் அரைக் காத அகலமும் உள்ளது.',\n",
       " 'காலப்போக்கில் அதன் பெயர் சிதைந்து இந்நாளில் வீராணத்து ஏரி என்ற பெயரால் வழங்கி வருகிறது.',\n",
       " 'புது வெள்ளம் வந்து பாய்ந்து ஏரியில் நீர் நிரம்பித் ததும்பி நிற்கும் ஆடி ஆவணி மாதங்களில் வீரநாராயண ஏரியைப் பார்ப்பவர் எவரும் நம்முடைய பழந்தமிழ் நாட்டு முன்னோர்கள் தங்கள் காலத்தில் சாதித்த அரும்பெரும் காரியங்களைக் குறித்துப் பெருமிதமும் பெரு வியப்பும் கொள்ளாமலிருக்க முடியாது.',\n",
       " 'நம் மூதாதையர்கள் தங்களுடைய நலனுக்கும் தங்கள் காலத்திய மக்களின் நலனுக்கும் உரிய காரியங்களை மட்டூமா செய்தார்கள்.',\n",
       " 'தாய்த் திருநாட்டில் தங்களுக்குப் பிற்காலத்தில் வாழையடி வாழையாக வரப்போகும் ஆயிரங்கால சந்ததிகளுக்கும் நன்மை பயக்கும் மாபெரும் செயல்களை நிறைவேற்றி விட்டுப் போனார்கள் அல்லவா.',\n",
       " 'ஆடித் திங்கள் பதினெட்டாம் நாள் முன் மாலை நேரத்தில் அலை கடல் போல் விரிந்து பரந்திருந்த வீர நாராயண ஏரிக்கரை மீது ஒரு வாலிப வீரன் குதிரை ஏறிப் பிரயாணம் செய்து கொண்டிருந்தான்.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a540d6",
   "metadata": {},
   "source": [
    "We are going to tokenize the text and add start and end tokens:\n",
    "\n",
    "- <b>Tokenize the text</b>:\n",
    "        We’ll split the text into sentences using tokenize.sent_tokenize() and count the total number of sentences.\n",
    "\n",
    "- <b>Add start and end tokens:</b>\n",
    "        For each sentence, we’ll add SENTENCE_START at the beginning and SENTENCE_END at the end to help the model understand sentence boundaries.\n",
    "\n",
    "- <b>Example output:</b>\n",
    "        We’ll print the first 10 tokenized sentences to verify that everything is working as expected.\n",
    "\n",
    "\n",
    "Now we are going to clean and tokenize the sentences:\n",
    "\n",
    "- <b>Remove unwanted punctuation:</b>\n",
    "        We’ll remove periods from the tokenized sentences while keeping the sentence boundaries intact. This ensures we don’t lose important punctuation like SENTENCE_START and SENTENCE_END.\n",
    "- <b>Count word frequencies:</b>\n",
    "        Using Counter, we’ll count how often each word appears in the text and print the total number of unique word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ad2aca-b19e-4450-8105-ba14493e8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21129 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenize and build vocabulary\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Add SENTENCE_START and SENTENCE_END tokens\n",
    "tokenized_sentences = [[sentence_start_token] + sentence + [sentence_end_token] for sentence in tokenized_sentences]\n",
    "# Flatten tokenized sentences to get all words\n",
    "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"Found {len(word_freq)} unique word tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed06bd5-59a3-4e51-a39d-05ff90ca95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected vocabulary size: 16137 with 95.0% coverage\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary to cover 95% of the text\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "total_word_count = sum(word_freq.values())\n",
    "coverage = 0\n",
    "vocab_size = 0\n",
    "desired_coverage = 0.95\n",
    "for word, count in word_freq.most_common():\n",
    "    coverage += count / total_word_count\n",
    "    vocab_size += 1\n",
    "    if coverage >= desired_coverage:\n",
    "        break\n",
    "\n",
    "print(f\"Selected vocabulary size: {vocab_size} with {desired_coverage * 100}% coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8323a",
   "metadata": {},
   "source": [
    "1. We sort the words by frequency and select the most common ones to cover 95% of the total word occurrences, determining the vocabulary size.  \n",
    "2. Mappings (`index_to_word` and `word_to_index`) are created for the vocabulary, including an `unknown_token` for out-of-vocabulary words.  \n",
    "3. Rare words in the tokenized sentences are replaced with `UNKNOWN_TOKEN` to ensure consistency during training.  \n",
    "4. An example sentence is shown with rare words replaced to verify how sentences look after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360a9be6-ebfa-430a-b0df-d936f702454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence after replacing rare words: ['SENTENCE_START', 'பொன்னியின்', 'செல்வன்', 'வரலாற்றுப்', 'புதினம்', 'அமரர்', 'கல்கி', 'கிருஷ்ணமூர்த்தி', 'அத்தியாயம்', 'ஆடித்திருநாள்', 'ஆதி', 'அந்தமில்லாத', 'கால', 'வெள்ளத்தில்', 'கற்பனை', 'ஓடத்தில்', 'ஏறி', 'நம்முடன்', 'சிறிது', 'நேரம்', 'பிரயாணம்', 'செய்யுமாறு', 'நேயர்களை', 'அழைக்கிறோம்', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# Create mappings from word to index and index to word\n",
    "vocab = word_freq.most_common(vocab_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = {word: i for i, word in enumerate(index_to_word)}\n",
    "\n",
    "# Replace words not in the vocabulary with UNKNOWN_TOKEN\n",
    "tokenized_sentences = [[word if word in word_to_index else unknown_token for word in sentence] for sentence in tokenized_sentences]\n",
    "tokenized_sentences = [\n",
    "    [word for word in sentence if word != '.']\n",
    "    for sentence in tokenized_sentences\n",
    "]\n",
    "# Show an example sentence after rare word handling\n",
    "print(f\"Example sentence after replacing rare words: {tokenized_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c8595e-0ced-432f-8191-13054f08f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentence: ['SENTENCE_START', 'இப்படியாவது', 'இராஜ்யம்', 'சம்பாதிக்க', 'வேண்டுமா', 'SENTENCE_END']\n",
      "Sentence as indices: [1, 13210, 2748, 13211, 2082, 2]\n",
      "Sentence from indices: ['SENTENCE_START', 'இப்படியாவது', 'இராஜ்யம்', 'சம்பாதிக்க', 'வேண்டுமா', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select a random sentence from tokenized_sentences\n",
    "random_sentence = random.choice(tokenized_sentences)\n",
    "\n",
    "# Convert the sentence to indices using word_to_index\n",
    "sentence_indices = [word_to_index[word] for word in random_sentence]\n",
    "\n",
    "# Convert the indices back to words using index_to_word\n",
    "sentence_words = [index_to_word[index] for index in sentence_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Random sentence:\", random_sentence)\n",
    "print(\"Sentence as indices:\", sentence_indices)\n",
    "print(\"Sentence from indices:\", sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef669d3c",
   "metadata": {},
   "source": [
    "## Step 3: Generating N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e3ce8",
   "metadata": {},
   "source": [
    "Range of n-grams: We are harvesting all n-grams from bigrams (2-grams) to 20-grams by iterating through lengths from 2 to 20.\n",
    "\n",
    "Count n-grams: For each length i, the function ngrams() is used to generate all possible n-grams of that length from the text. The Counter is then used to count the occurrences of each n-gram.\n",
    "\n",
    "Store n-grams: The counts for each n-gram length are printed and stored in the ngrams_up_to_20 list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcdd4e6-eb11-4923-ae41-9cfe374e3f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram-2 length: 326762\n",
      "ngram-3 length: 416139\n",
      "ngram-4 length: 427866\n",
      "ngram-5 length: 429198\n",
      "ngram-6 length: 429397\n",
      "ngram-7 length: 429440\n",
      "ngram-8 length: 429454\n",
      "ngram-9 length: 429456\n",
      "ngram-10 length: 429457\n",
      "ngram-11 length: 429457\n",
      "ngram-12 length: 429456\n",
      "ngram-13 length: 429455\n",
      "ngram-14 length: 429454\n",
      "ngram-15 length: 429453\n",
      "ngram-16 length: 429452\n",
      "ngram-17 length: 429451\n",
      "ngram-18 length: 429450\n",
      "ngram-19 length: 429449\n",
      "ngram-20 length: 429448\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Harvesting all n-grams up to length 20\n",
    "ngrams_up_to_20 = []\n",
    "for i in range(2, 21):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))  # Collecting n-grams of length i\n",
    "    print(f'ngram-{i} length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf0aca",
   "metadata": {},
   "source": [
    "We need to ensure that the n-grams we keep are complete and not broken by sentence-ending punctuation. So, we will implement helper functions to enure that. They are:\n",
    "- remove_periods(): This function checks if any word in the n-gram contains a period or quotation mark. If any such characters are found, the function returns False, indicating that the n-gram should be excluded.\n",
    "\n",
    "- my_filter(): This function applies remove_periods() to a list of n-grams, filtering out any n-grams that span sentence boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b275f1-9f28-4ed9-b48a-55de10392a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove n-grams that contain periods or quotes\n",
    "def remove_periods(ngram):\n",
    "    \"\"\"Remove n-grams that contain periods or quotes.\"\"\"\n",
    "    for word in ngram[0]:\n",
    "        if '.' in word or '’' in word or '‘' in word:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Keep only repeating n-grams\n",
    "def my_filter(ngrams):\n",
    "    \"\"\"Filter n-grams to only keep those that occur more than once and do not span sentence boundaries.\"\"\"\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645eb86",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eaff7",
   "metadata": {},
   "source": [
    "Now, we'll construct the training dataset (X_train and y_train) using n-grams from 2-grams to 20-grams:\n",
    "\n",
    "- Initialize training data: Empty lists X_train and y_train are created to store the input sequences and target words.\n",
    "\n",
    "- Process n-grams: For each set of n-grams (from bigrams to 20-grams), we iterate through the most common n-grams.\n",
    "\n",
    "- Filter valid n-grams: Using my_filter(), we ensure all n-grams pass certain conditions, and only those where all words are in the vocabulary (word_to_index) are considered.\n",
    "\n",
    "- Create training examples:\n",
    "        X_train: The input sequence consists of the n-gram minus the last word.\n",
    "        y_train: The target is the last word of the n-gram.\n",
    "\n",
    "- Final count: The total number of sequences generated from n-grams is printed, showing how many training examples were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532fcee3-dc59-4348-a6ac-e239de8f4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences from n-grams: 340256\n"
     ]
    }
   ],
   "source": [
    "# Initialize training data lists\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process all n-grams from 2 to 20\n",
    "for i in range(len(ngrams_up_to_20)):  # Starting from bigrams\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "\n",
    "    # Construct X_train and y_train using the filtered n-grams\n",
    "    for sent in my_filter(ngrams_to_learn.most_common()):\n",
    "        ngram = sent[0]\n",
    "        # Ensure all words are in vocabulary\n",
    "        if all(word in word_to_index for word in ngram):\n",
    "            ngram_indices = [word_to_index[word] for word in ngram]\n",
    "            # Input sequence is the n-gram minus the last word\n",
    "            X_train.append(ngram_indices[:-1])\n",
    "            # Target is the last word of the n-gram\n",
    "            y_train.append(ngram_indices[-1])\n",
    "\n",
    "print(f'Total sequences from n-grams: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0155930",
   "metadata": {},
   "source": [
    "We now expand the training dataset by incorporating sequences from complete tokenized sentences. For each sentence, we ensure that all words are in the vocabulary, and then we create input sequences using the words leading up to the current word, with the current word as the target output. This allows the model to learn from entire sentence structures, improving its ability to predict the next word based on the broader context of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc62717-31f6-4fe9-847c-2ffe32bb5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences after including sentences: 421996\n"
     ]
    }
   ],
   "source": [
    "# Include sequences from your tokenized sentences\n",
    "for sentence in tokenized_sentences:\n",
    "    if all(word in word_to_index for word in sentence):\n",
    "        sentence_indices = [word_to_index[word] for word in sentence]\n",
    "        for i in range(1, len(sentence_indices)):\n",
    "            X_train.append(sentence_indices[:i])\n",
    "            y_train.append(sentence_indices[i])\n",
    "\n",
    "print(f'Total sequences after including sentences: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fcbcd",
   "metadata": {},
   "source": [
    "We combine X_train (input sequences) and y_train (target words) into a single list of tuples, then shuffle them together using random.shuffle().\n",
    "After shuffling, we unpack the combined list back into X_train and y_train, keeping the pairs aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d42a0a1-a1ff-44d5-af6b-d490581c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "combined = list(zip(X_train, y_train))\n",
    "random.shuffle(combined)\n",
    "X_train[:], y_train[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0555b4a",
   "metadata": {},
   "source": [
    "we are preparing the training data by padding the sequences and converting them into the appropriate format for model training:\n",
    "\n",
    "- Determine maximum sequence length: We calculate the length of the longest sequence in X_train to use this as the standard for padding all sequences to the same length.\n",
    "\n",
    "- Pad sequences: Using pad_sequences(), we pad the input sequences (X_train) with zeros at the beginning (pre-padding). This ensures that all sequences have the same length, which is necessary for training models that expect fixed-length input.\n",
    "\n",
    "- Convert y_train to a NumPy array: We convert y_train to a NumPy array, making it compatible with machine learning libraries that require data in this format.\n",
    "\n",
    "- Print shape of padded data: We print the shape of the padded X_train and y_train to verify that they are ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d592d56-b426-4105-afc9-7baecace42a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 75\n",
      "X_train_padded shape: (421996, 75), y_train shape: (421996,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in X_train)\n",
    "print(f'Max sequence length: {max_seq_length}')\n",
    "\n",
    "# Pad sequences with zeros at the beginning\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# Convert y_train to a NumPy array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f'X_train_padded shape: {X_train_padded.shape}, y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389be1ac",
   "metadata": {},
   "source": [
    "We are splitting the dataset into training and validation sets. Using train_test_split(), 90% of the data is allocated for training, while 10% is reserved for validation. The validation set helps us assess the model's performance on unseen data, ensuring it generalizes well beyond the training set. The split is made reproducible by setting a random seed (random_state=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "185ed9ee-9f55-45da-8bf2-b0e4e355983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (379796, 75), (379796,)\n",
      "Validation data shape: (42200, 75), (42200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train_padded_train, X_train_padded_val, y_train_train, y_train_val = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training data shape: {X_train_padded_train.shape}, {y_train_train.shape}')\n",
    "print(f'Validation data shape: {X_train_padded_val.shape}, {y_train_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3cc63f",
   "metadata": {},
   "source": [
    "## Step 5: Saving all the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e287140",
   "metadata": {},
   "source": [
    "Since this is a model that takes extensive tuning and training, we are saving the processed training data and vocabulary to disk using pickle.\n",
    "\n",
    "The main idea behind pickling is to avoid having to redo the entire preprocessing each time we want to use the data. By saving X_train, y_train, the tokenized sentences, and the word-to-index and index-to-word mappings, we can easily reload them later. \n",
    "\n",
    "This makes sure that we always have access to all the necessary data for training, predictions and further analysis, as it can be loaded quickly without re-running the entire data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3be1854-89dd-4a3e-9809-42aace6296b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the processed data\n",
    "with open('pickle/X_train_padded_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_padded, file)\n",
    "with open('pickle/y_train_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "with open('pickle/tokenized_sentences_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenized_sentences, file)\n",
    "with open('pickle/word_to_index_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(word_to_index, file)\n",
    "with open('pickle/index_to_word_ps.pkl', 'wb') as file:\n",
    "    pickle.dump(index_to_word, file)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b46d78-10be-4aad-a204-48016aae65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CBOW embeddings from your saved file\n",
    "# embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# # Open your CBOW embedding file\n",
    "# with open('my_cbow_vectors_ps.txt', 'r', encoding='utf-8') as f:\n",
    "#     # Read the header\n",
    "#     vocab_size, embedding_dim = map(int, f.readline().split())\n",
    "    \n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]  # The word itself\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "#         embeddings_index[word] = coefs\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Create the embedding matrix for the CBOW embeddings\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# for word, i in word_to_index.items():\n",
    "#     if i >= vocab_size:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e2540",
   "metadata": {},
   "source": [
    "## Step 6: Loading our pre-trained CBoW Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da74b0",
   "metadata": {},
   "source": [
    "Now, we are loading pre-trained word embeddings from our previous notebooks where we created embeddings using CbOw on Ponniyn Selvan into a dictionary called embeddings_index. \n",
    "- Each word from the CBoW file is mapped to its corresponding vector representation, which captures its semantic meaning in a numerical form. \n",
    "- By storing these embeddings, we can later use them to initialize the word representations in our model, allowing it to leverage the rich, pre-learned relationships between words, which can improve the model's performance, especially when dealing with limited training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669fea16-54d1-4d32-b20f-a5c0457b7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69351 word vectors.\n",
      "16137\n",
      "Embedding matrix shape: (16137, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# Open the GloVe embeddings file\n",
    "with open('my_cbow_vectors_ps.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # The word itself\n",
    "        coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "print(vocab_size)\n",
    "for word, i in word_to_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65268fde",
   "metadata": {},
   "source": [
    "## Step 7: Building our Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad5881",
   "metadata": {},
   "source": [
    "We define an RNN model using Keras with two LSTM layers for text sequence prediction:\n",
    "- Input Layer: Takes sequences of predefined maximum length.\n",
    "- Embedding Layer: Uses pre-trained CBoW embeddings, which are non-trainable, for initializing word vectors.\n",
    "- LSTM Layers: Includes two LSTM layers with 64 hidden units each. To prevent overfitting and stabilize training, each LSTM layer is followed by a Dropout layer and BatchNormalization.\n",
    "- Output Layer: A Dense layer with softmax activation outputs probabilities for the next word.\n",
    "- ReduceLROnPlateau: Configures ReduceLROnPlateau to decrease the learning rate when the model's loss plateaus, aiding in convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b89774-db77-47d8-a799-abf5e6c59c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_rnn_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, 75, 100)           1613700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75, 64)            42240     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_norm_1 (BatchNormaliza (None, 75, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_norm_2 (BatchNormaliza (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 16137)             1048905   \n",
      "=================================================================\n",
      "Total params: 2,738,381\n",
      "Trainable params: 1,124,425\n",
      "Non-trainable params: 1,613,956\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Hidden dimensions for LSTM layers\n",
    "hidden_dim = 64\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=(max_seq_length,), name='input_layer')\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False,\n",
    "    name='embedding_layer'\n",
    ")(inputs)\n",
    "\n",
    "# First LSTM Block\n",
    "lstm1 = LSTM(units=hidden_dim, return_sequences=True, name='lstm_1')(embedding)\n",
    "dropout1 = Dropout(0.1, name='dropout_1')(lstm1)\n",
    "bn1 = BatchNormalization(name='batch_norm_1')(dropout1)\n",
    "\n",
    "# Second LSTM Block\n",
    "lstm2 = LSTM(units=hidden_dim, name='lstm_2')(bn1)\n",
    "dropout2 = Dropout(0.1, name='dropout_2')(lstm2)\n",
    "bn2 = BatchNormalization(name='batch_norm_2')(dropout2)\n",
    "\n",
    "# Output Layer\n",
    "outputs = Dense(vocab_size, activation='softmax', name='output_layer')(bn2)\n",
    "\n",
    "# Define the Model\n",
    "model = Model(inputs=inputs, outputs=outputs, name='functional_rnn_model')\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam()  # You can specify learning rate or other parameters if needed\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "# Define the learning rate reduction callback\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.5,\n",
    "    patience=1,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6  # Optional: set a minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0b60f",
   "metadata": {},
   "source": [
    "## Step 8: Training our RNN Model on Ponniyn Selvan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9900c5",
   "metadata": {},
   "source": [
    "We are training the model using the fit method, where the training data (X_train_padded_train and y_train_train) is used to adjust the model's weights over 150 epochs, with a batch size of 128. The validation data (X_train_padded_val and y_train_val) is used to monitor the model's performance on unseen data during training. The ReduceLROnPlateau callback reduces the learning rate if the loss stops improving, helping the model converge more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d41ba32-292e-4f37-ba05-9f62f1c3f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 379796 samples, validate on 42200 samples\n",
      "Epoch 1/50\n",
      "379796/379796 [==============================] - 621s 2ms/step - loss: 7.6343 - val_loss: 6.7252\n",
      "Epoch 2/50\n",
      "379796/379796 [==============================] - 640s 2ms/step - loss: 6.4277 - val_loss: 5.9692\n",
      "Epoch 3/50\n",
      "379796/379796 [==============================] - 626s 2ms/step - loss: 5.8439 - val_loss: 5.6016\n",
      "Epoch 4/50\n",
      "379796/379796 [==============================] - 639s 2ms/step - loss: 5.5250 - val_loss: 5.4070\n",
      "Epoch 5/50\n",
      "379796/379796 [==============================] - 642s 2ms/step - loss: 5.3226 - val_loss: 5.2677\n",
      "Epoch 6/50\n",
      "379796/379796 [==============================] - 645s 2ms/step - loss: 5.1765 - val_loss: 5.1609\n",
      "Epoch 7/50\n",
      "379796/379796 [==============================] - 646s 2ms/step - loss: 5.0686 - val_loss: 5.0957\n",
      "Epoch 8/50\n",
      "379796/379796 [==============================] - 652s 2ms/step - loss: 4.9836 - val_loss: 5.0370\n",
      "Epoch 9/50\n",
      "379796/379796 [==============================] - 647s 2ms/step - loss: 4.9121 - val_loss: 4.9938\n",
      "Epoch 10/50\n",
      "379796/379796 [==============================] - 652s 2ms/step - loss: 4.8539 - val_loss: 4.9454\n",
      "Epoch 11/50\n",
      "379796/379796 [==============================] - 648s 2ms/step - loss: 4.8022 - val_loss: 4.9109\n",
      "Epoch 12/50\n",
      "379796/379796 [==============================] - 647s 2ms/step - loss: 4.7607 - val_loss: 4.8899\n",
      "Epoch 13/50\n",
      "379796/379796 [==============================] - 646s 2ms/step - loss: 4.7202 - val_loss: 4.8657\n",
      "Epoch 14/50\n",
      "379796/379796 [==============================] - 656s 2ms/step - loss: 4.6898 - val_loss: 4.8388\n",
      "Epoch 15/50\n",
      "379796/379796 [==============================] - 650s 2ms/step - loss: 4.6601 - val_loss: 4.8209\n",
      "Epoch 16/50\n",
      "379796/379796 [==============================] - 650s 2ms/step - loss: 4.6340 - val_loss: 4.8013\n",
      "Epoch 17/50\n",
      "379796/379796 [==============================] - 643s 2ms/step - loss: 4.6122 - val_loss: 4.7871\n",
      "Epoch 18/50\n",
      "379796/379796 [==============================] - 641s 2ms/step - loss: 4.5865 - val_loss: 4.7868\n",
      "Epoch 19/50\n",
      "379796/379796 [==============================] - 641s 2ms/step - loss: 4.5669 - val_loss: 4.7625\n",
      "Epoch 20/50\n",
      "379796/379796 [==============================] - 654s 2ms/step - loss: 4.5490 - val_loss: 4.7532\n",
      "Epoch 21/50\n",
      "379796/379796 [==============================] - 640s 2ms/step - loss: 4.5337 - val_loss: 4.7367\n",
      "Epoch 22/50\n",
      "379796/379796 [==============================] - 646s 2ms/step - loss: 4.5217 - val_loss: 4.7276\n",
      "Epoch 23/50\n",
      "379796/379796 [==============================] - 645s 2ms/step - loss: 4.5027 - val_loss: 4.7193\n",
      "Epoch 24/50\n",
      "379796/379796 [==============================] - 656s 2ms/step - loss: 4.4923 - val_loss: 4.7152\n",
      "Epoch 25/50\n",
      "379796/379796 [==============================] - 649s 2ms/step - loss: 4.4725 - val_loss: 4.7020\n",
      "Epoch 26/50\n",
      "379796/379796 [==============================] - 648s 2ms/step - loss: 4.4631 - val_loss: 4.6938\n",
      "Epoch 27/50\n",
      "379796/379796 [==============================] - 657s 2ms/step - loss: 4.4547 - val_loss: 4.6898\n",
      "Epoch 28/50\n",
      "379796/379796 [==============================] - 646s 2ms/step - loss: 4.4457 - val_loss: 4.6795\n",
      "Epoch 29/50\n",
      "379796/379796 [==============================] - 650s 2ms/step - loss: 4.4339 - val_loss: 4.6754\n",
      "Epoch 30/50\n",
      "379796/379796 [==============================] - 643s 2ms/step - loss: 4.4235 - val_loss: 4.6758\n",
      "Epoch 31/50\n",
      "379796/379796 [==============================] - 649s 2ms/step - loss: 4.4168 - val_loss: 4.6632\n",
      "Epoch 32/50\n",
      "379796/379796 [==============================] - 645s 2ms/step - loss: 4.4051 - val_loss: 4.6624\n",
      "Epoch 33/50\n",
      "379796/379796 [==============================] - 641s 2ms/step - loss: 4.3987 - val_loss: 4.6542\n",
      "Epoch 34/50\n",
      "379796/379796 [==============================] - 644s 2ms/step - loss: 4.3919 - val_loss: 4.6506\n",
      "Epoch 35/50\n",
      "379796/379796 [==============================] - 646s 2ms/step - loss: 4.3865 - val_loss: 4.6460\n",
      "Epoch 36/50\n",
      "379796/379796 [==============================] - 650s 2ms/step - loss: 4.3757 - val_loss: 4.6457\n",
      "Epoch 37/50\n",
      "379796/379796 [==============================] - 642s 2ms/step - loss: 4.3714 - val_loss: 4.6380\n",
      "Epoch 38/50\n",
      "379796/379796 [==============================] - 648s 2ms/step - loss: 4.3676 - val_loss: 4.6297\n",
      "Epoch 39/50\n",
      "379796/379796 [==============================] - 651s 2ms/step - loss: 4.3616 - val_loss: 4.6239\n",
      "Epoch 40/50\n",
      "379796/379796 [==============================] - 659s 2ms/step - loss: 4.3511 - val_loss: 4.6283\n",
      "Epoch 41/50\n",
      "379796/379796 [==============================] - 647s 2ms/step - loss: 4.3461 - val_loss: 4.6210\n",
      "Epoch 42/50\n",
      "379796/379796 [==============================] - 649s 2ms/step - loss: 4.3395 - val_loss: 4.6164\n",
      "Epoch 43/50\n",
      "379796/379796 [==============================] - 655s 2ms/step - loss: 4.3361 - val_loss: 4.6219\n",
      "Epoch 44/50\n",
      "379796/379796 [==============================] - 662s 2ms/step - loss: 4.3298 - val_loss: 4.6158\n",
      "Epoch 45/50\n",
      "379796/379796 [==============================] - 662s 2ms/step - loss: 4.3237 - val_loss: 4.6128\n",
      "Epoch 46/50\n",
      "379796/379796 [==============================] - 670s 2ms/step - loss: 4.3230 - val_loss: 4.6051\n",
      "Epoch 47/50\n",
      "379796/379796 [==============================] - 669s 2ms/step - loss: 4.3182 - val_loss: 4.6084\n",
      "Epoch 48/50\n",
      "379796/379796 [==============================] - 663s 2ms/step - loss: 4.3135 - val_loss: 4.5973\n",
      "Epoch 49/50\n",
      "379796/379796 [==============================] - 664s 2ms/step - loss: 4.3085 - val_loss: 4.6014\n",
      "Epoch 50/50\n",
      "379796/379796 [==============================] - 658s 2ms/step - loss: 4.3064 - val_loss: 4.5984\n"
     ]
    }
   ],
   "source": [
    "# Now retry training the model\n",
    "history = model.fit(\n",
    "    X_train_padded_train, y_train_train,\n",
    "    validation_data=(X_train_padded_val, y_train_val),\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    callbacks=[reduce_lr]  # Include the learning rate callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f747b0",
   "metadata": {},
   "source": [
    "## Step 9: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712cdc59-b00e-46e5-85d0-f8c1ecd4611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights using pickle\n",
    "# with open('tensorflow_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "model.save('model/rnn_cbow_model.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058c391",
   "metadata": {},
   "source": [
    "## Step 10: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb293d",
   "metadata": {},
   "source": [
    "#### Generate Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29ab09",
   "metadata": {},
   "source": [
    "The generate_sentence() function uses the trained RNN model to generate sentences. It starts with the sentence_start_token and predicts each subsequent word by sampling from the model's probability distribution for the next word. The generation continues until it reaches the sentence_end_token or a specified sentence length. The predicted word indices are then converted back into actual words, forming a complete sentence. This function enables the model to create coherent text based on the patterns it has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a724a4f-e53d-4c4b-939b-99f59d7944cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, word_to_index, index_to_word, max_seq_length, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the maximum sentence length\n",
    "    while (new_sentence[-1] != word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word, avoiding UNKNOWN_TOKEN\n",
    "        sampled_word_index = word_to_index[unknown_token]\n",
    "        while sampled_word_index == word_to_index[unknown_token]:\n",
    "            # Sample from the distribution\n",
    "            sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Append the sampled word to the sentence\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices to words, excluding SENTENCE_START and SENTENCE_END tokens\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence[1:-1]]\n",
    "    return ' '.join(sentence_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73576f41",
   "metadata": {},
   "source": [
    "The generate_sentence() function uses the trained RNN model to generate sentences. It starts with the sentence_start_token and predicts each subsequent word by sampling from the model's probability distribution for the next word. The generation continues until it reaches the sentence_end_token or a specified sentence length. The predicted word indices are then converted back into actual words, forming a complete sentence. This function enables the model to create coherent text based on the patterns it has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59ee50de-b4d9-4841-b4af-b9a8d708dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "” “தாங்கள் கவலைப்படவில்லை மலையமான் மகள் வானவன்மாதேவிக்கும் தாதியர்களுக்கும் ஒரு பெரிய மரத்தின் அடியில் இறங்கினார்களோ _ comma _ காடூ சுற்றிச் சுற்றி\n",
      "_ comma _ பல இள மரங்களின் மாதரசி\n",
      "நான் அவ்விதம் செய்வதற்கு பெரும் போர் விட்டூ வருகிறார்கள்\n",
      "படூத்துக் மிகப் பக்கம் சகிக்காமல் ஆர்வம் ஆதித்த கரிகாலரின் உயிருக்கு இப்போது நம்மிடம் உயிர்க்குயிரான நன்கு உணர்ந்து கொண்டேன்\n",
      "அல்லது பனித்துளி அனைவரும் வீடுகளும் கடைவீதிகளும் சிவாலயக் கற்றளிகளும் திருமாலுக்குரிய விண்ணகரங்களும் குரல்\n",
      "கம்பீரமான சோழ மன்னரின் இளம் புதல்வர் அருள்மொழிவர்மர் முன்வந்தார் “அப்பா என்று கட்டளையிட்டார் _ comma _ _ comma _ ஆனால் கதை\n",
      "ஆண்டாளின் பெற்ற பட்டத்து பராந்தக சக்கரவர்த்தி அருமை மூத்த புதல்வரிடமிருந்து கடிதங்களும் நான் என்ன அவசர காரியம் இல்லை\n",
      "குந்தவையும் ஒற்றன் அதோ நாட்டில் தமிழ்ப் பாடலுக்குப் பசித்திருக்கின்றன\n",
      "சைவர்களும் வைஷ்ணவர்களும் என்று என்னை _ comma _ கடம்பூர் சம்புவரையர் மகனை கூறினார்\n",
      "காடூம் உன்னைக் மீண்டும் சோழ குலத்தின் செய்தார்கள் எதற்காகத் இங்கே அழைத்து வரச்\n",
      "மாட்டீர்கள் அந்தரங்க நினைத்துப் பார்த்தால் _ comma _ பல இள _ comma _ அழகில் மன்மதனுக்கு ஒப்பானவர்\n",
      "” “ஓகோ சுந்தர சோழர் மேலும் மேலும் மேலும் கூறினான் காதில் விழுந்ததும் நானும் வடபெண்ணைக்கரைப் பாசறையில் எல்லைக் காவல் புரிந்து கொண்டிருந்தோம்\n",
      "வீட்டுக் கூரைகளின் வெகுவாகக் மீது குற்றம் ஏற்பட்டால் சக்கரவர்த்தியே _ comma _ தம்பி நல்லது _ comma _ இருக்கிறார்\n",
      "உன் மிகுந்த அவயங்களுடன் ஒப்பிடத் ஒப்பிடத் கூட்டம் முன் குடூமியைச் சிரைத்துத் பெயர் மிக அது என்று தெரிந்து கொண்டு அதைக் வேண்டூம்\n",
      "கலிங்க தேசத்து ராஜாவும் வடக்கே சென்றார்கள் என்பது முகத்திலிருந்து நிம்மதி ஆடூம் மூலம் மக்கள் “சுந்தர இதற்குமுன் இருந்து தவம்\n",
      "எதற்காகக் _ comma _ தங்களுக்கு _ comma _ உள்ளே சென்று தம்மை என்று விசாரித்துவர விட்டூ விட்டு வேறு மகிழ்ச்சி\n",
      "அதைக் வந்தனத்துடன் அல்லவா _ comma _ “தம்பி நீ ஆனால் கழுவில் இரு உருவங்கள்\n",
      "இவர்கள் கிருஷ்ணனைப் சமயத்தில் முத்திரை மோதிரம் மறுமொழி என்ன பிசகு\n",
      "நிச்சயமாய்த் அவர்கள் உமக்கு நம் அரண்மனைக்கு அருகில் வரச்\n",
      "தன் உதவியைக் திடீரென்று உண்மையிலேயே திருப்பணி செய்து கொண்டிருந்தபோது வீரச் செயல்களை மறைந்து விட்டது\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    sent = ''\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent.split()) < senten_min_length:\n",
    "        sent = generate_sentence(model, word_to_index, index_to_word, max_seq_length, senten_max_length)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2831fd",
   "metadata": {},
   "source": [
    "#### Generating based on a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2e184",
   "metadata": {},
   "source": [
    "The generate_sentence() function generates a sentence based on a given starting text. It first converts the starting words into word indices using the model's vocabulary. Then, it uses the trained RNN model to predict and append the next word, continuing until the sentence reaches a specified maximum length or the end token is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd6e2d07-5d64-40ee-8213-55807bed9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_text, senten_max_length):\n",
    "    # Convert the start_text to lowercase to match your vocabulary\n",
    "    start_text = start_text.lower()\n",
    "    \n",
    "    # Convert the start_text into indices based on your vocabulary\n",
    "    new_sentence = [word_to_index.get(word, word_to_index[unknown_token]) for word in start_text.split()]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the max sentence length\n",
    "    while len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word\n",
    "        sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Check if the sampled word is the SENTENCE_END token\n",
    "        if sampled_word_index == word_to_index.get(sentence_end_token):\n",
    "            break  # Stop adding words if we reach the end token\n",
    "        \n",
    "        # Append the predicted word\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence]\n",
    "    generated_text = ' '.join(sentence_str)\n",
    "    generated_text += '.'\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2b430",
   "metadata": {},
   "source": [
    "#### <b> Prompts and their Outputs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78cb1f34-48dd-4c5a-9313-6d934a6c509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: பொதுவாக மலரின் இனிய ஜங்கார சுந்தர சோழரும் சுந்தர சோழ சக்கரவர்த்தி தர்ம ராஜ்யம் நடப்பதாகச் பார்த்துக் கொண்டிருந்த பெரும் பெரிய நதியில் நதி வரையிலும் வரையிலும் பரந்திருந்த தேசங்களிலிருந்து தலைநகருக்குப் பலர் திரும்பிப் பார்த்து அவளை என் காதில் விழுந்து அவருக்குச் முழுவதும் காது கொடுத்துக் கவனமாகக் கேட்டபோது என் தந்தை பார்த்து இன்று வரை வந்து விட்டதோ என்று இருக்கின்றன அல்லவா நீ சொன்னபடி வேலை செய்து.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"பொதுவாக\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eff72c9-8bb5-4f91-9ac3-328e0c761efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: வணக்கம் செய்துவிட்டுப் பாடவும் ஆடவும் தொடங்கினார்கள் என்று பல குரல்களில் முன்னால் கனவு செய்ய வேண்டும் என்று நண்பர்கள் இருவரும் அங்கிருந்து போய் விட்டுப் போவது என்று எனக்கே தெரியவில்லை கூடியவர்கள் சொல்லுங்கள் பார்த்து சொல்லி விட்டூ விட்டூ என்னைக் கொண்டுதான் தெரிந்து கொண்டூ போக முடியாது என்பதை பழுவூர் இளைய ராணியின் கோட்டைக் அழைத்து வரச் அவரைப் பார்த்து விட்டு திரும்பி வருவான் என்று சொல்ல சொல்ல வேண்டும்.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"வணக்கம்\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ec406fc-f04c-4eac-a955-c3e74665636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: சோழன் அவளுடைய உதவியை அடியோடு விட்டூ விட்டு விட்டூ அனைவரும் நிம்மதி பல வர்ண இறகுகள் படைத்த பட்டுப் பூச்சி ஒருவன் வந்து மீண்டும் வழங்கும் இப்பாடல்களில் கடைசிப் சைவர் என்று பெயர் வந்தியத்தேவன் கூறியதும் கருணை சந்நியாசி அருகில் வந்து நின்று வண்ணம் பார்த்துக் கொண்டு விரைந்து கரையேறத் செய்து கொண்டூ புன்னகை வந்து புன்னகை புரிந்து கொண்டிருப்பது போல் இந்தச் சிங்கள மன்னர்களுக்குப் புத்தி கற்பிக்க வேண்டும்.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"சோழன்\"\n",
    "generated_output = generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6683e6c-9aa2-4542-acf0-cd98ed965be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('model/rnn_cbow_model_weight.h5')\n",
    "print('Model Saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
