{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c9aef3",
   "metadata": {},
   "source": [
    "# 4. Ponniyn Selvan Tamil Embeddings using DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516579d6",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfb2f77-75fc-4958-8b48-d03e8947dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lokeshbalamurugan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# For Keras model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "from indicnlp.tokenize.sentence_tokenize import sentence_split\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df54b99",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c7916",
   "metadata": {},
   "source": [
    "We are going to set up tokens first:\n",
    "\n",
    "- unknown_token: This token will represent words that aren’t in our vocabulary. It helps the model manage unfamiliar words during training and generation.\n",
    "\n",
    "- sentence_start_token: This token will be added to the beginning of every sentence, so the model understands where sentences start.\n",
    "\n",
    "- sentence_end_token: This token will be placed at the end of each sentence, allowing the model to know when the sentence is complete.\n",
    "\n",
    "These tokens will help the model structure sentences and deal with unknown words effectively during training and text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ac42e4-4613-4fde-937d-d724609d419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e040c",
   "metadata": {},
   "source": [
    "We want to clean any numbers and roman numerals that might arise in the in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ee8742-6467-457b-a7b2-cbd59d7497d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    pattern = r\"[\\d-]\"\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def clean_roman_numerals(text):\n",
    "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb910f",
   "metadata": {},
   "source": [
    "Next, we are going to read and clean the text from The Invisible Man by H.G. Wells. The goal is to prepare the text so it can be fed into our model without unnecessary punctuation, chapter headings, or formatting issues. Here's how we'll do that:\n",
    "\n",
    "- <b>Reading the file:</b> We’ll open the text file (invisible_man_gutenberg.txt) and read its content into memory.\n",
    "\n",
    "- <b>Removing unwanted punctuation:</b>\n",
    "        We'll remove specific punctuation marks, such as commas, colons, quotes, and dashes, by creating a translation table.\n",
    "        We’ll also use regular expressions to keep sentence-ending punctuation like periods (.), question marks (?), and exclamation marks (!) but remove all other unwanted punctuation.\n",
    "\n",
    "- <b>Handling sentence-ending punctuation:</b>\n",
    "        We’ll replace ? and ! with periods (.) to normalize sentence ends, which is useful for consistent sentence boundaries during training.\n",
    "\n",
    "- <b>Removing chapter headings and titles:</b>\n",
    "        Chapter headings like \"CHAPTER I\" and other titles at the beginning of chapters will be removed to prevent the model from learning irrelevant text structures.\n",
    "\n",
    "- <b>Converting text to lowercase:</b>\n",
    "        By converting everything to lowercase, we ensure that words like \"Invisible\" and \"invisible\" are treated as the same word during training.\n",
    "\n",
    "- <b>Removing extra whitespace:</b>\n",
    "        We’ll also clean up any extra spaces or line breaks in the text so it’s uniformly formatted before being tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9a3a2f7-fa25-4a19-879e-be9932201632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading txt file...\n",
      "Total number of sentences: 8000\n",
      "Preprocessing done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading txt file...\")\n",
    "with open(r'data/ponniyin-selvan.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preprocessing: Replacing punctuation and cleaning\n",
    "text = text.replace(\",\\n\", \" _eol_ \")\n",
    "text = text.replace(\",\", \" _comma_ \")\n",
    "text = text.replace(\":\", \" _comma_ \")\n",
    "text = text.replace(\";\", \" _comma_ \")\n",
    "text = text.replace(\"?\\n\", \". \")\n",
    "text = text.replace(\"!\\n\", \". \")\n",
    "text = text.replace(\".\\n\", \". \")\n",
    "text = text.replace('\"', \"\")  # Remove double quotes\n",
    "text = text.replace(\"'\", \"\")  # Remove single quotes\n",
    "text = text.replace(\"?\", \".\")\n",
    "text = text.replace(\"!\", \".\")\n",
    "text = text.replace(\"\\t\", \"\")\n",
    "text = text.replace(\"\\u200c\", \"\")  # Remove zero-width non-joiner\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "\n",
    "# Additional cleaning\n",
    "text = clean_numbers(text)\n",
    "text = clean_roman_numerals(text)\n",
    "\n",
    "# Sentence splitting using indic-nlp-library for Tamil\n",
    "sentences = sentence_split(text, lang='ta')  # Tamil language code\n",
    "\n",
    "# Lowercase and tokenize the sentences\n",
    "sentences = [s.lower().strip() for s in sentences if len(s.split()) > 2]\n",
    "tokenized_sentences = [indic_tokenize.trivial_tokenize(s, lang='ta') for s in sentences]\n",
    "\n",
    "# Now, limit the corpus to the first 6,000 sentences\n",
    "num_sentences_to_use = 8000\n",
    "tokenized_sentences = tokenized_sentences[:num_sentences_to_use]\n",
    "\n",
    "print(f\"Total number of sentences: {len(tokenized_sentences)}\")\n",
    "\n",
    "print(\"Preprocessing done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc295271-741c-4966-82a3-0df78879732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['பொன்னியின் செல்வன் வரலாற்றுப் புதினம் அமரர் கல்கி கிருஷ்ணமூர்த்தி அத்தியாயம்   ஆடித்திருநாள் ஆதி அந்தமில்லாத கால வெள்ளத்தில் கற்பனை ஓடத்தில் ஏறி நம்முடன் சிறிது நேரம் பிரயாணம் செய்யுமாறு நேயர்களை அழைக்கிறோம்.',\n",
       " 'விநாடிக்கு ஒரு நூற்றாண்டூ வீதம் எளிதில் கடந்து இன்றைக்குத் தொள்ளாயிரத்து எண்பத்திரண்டூ (ல் எழுதியது) ஆண்டூகளுக்கு முந்திய காலத்துக்குச் செல்வோமாக.',\n",
       " 'தொண்டை நாட்டுக்கும் சோழ நாட்டுக்கும் இடையில் உள்ள திருமுனைப்பாடி நாட்டின் தென்பகுதியில் _comma_ தில்லைச் சிற்றம்பலத்துக்கு மேற்கே இரண்டூ காததூரத்தில் _comma_ அலை கடல் போன்ற ஓர் ஏரி விரிந்து பரந்து கிடக்கிறது.',\n",
       " 'அதற்கு வீரநாராயண ஏரி என்று பெயர்.',\n",
       " 'அது தெற்கு வடக்கில் ஒன்றரைக் காத நீளமும் கிழக்கு மேற்கில் அரைக் காத அகலமும் உள்ளது.',\n",
       " 'காலப்போக்கில் அதன் பெயர் சிதைந்து இந்நாளில் வீராணத்து ஏரி என்ற பெயரால் வழங்கி வருகிறது.',\n",
       " 'புது வெள்ளம் வந்து பாய்ந்து ஏரியில் நீர் நிரம்பித் ததும்பி நிற்கும் ஆடி ஆவணி மாதங்களில் வீரநாராயண ஏரியைப் பார்ப்பவர் எவரும் நம்முடைய பழந்தமிழ் நாட்டு முன்னோர்கள் தங்கள் காலத்தில் சாதித்த அரும்பெரும் காரியங்களைக் குறித்துப் பெருமிதமும் பெரு வியப்பும் கொள்ளாமலிருக்க முடியாது.',\n",
       " 'நம் மூதாதையர்கள் தங்களுடைய நலனுக்கும் தங்கள் காலத்திய மக்களின் நலனுக்கும் உரிய காரியங்களை மட்டூமா செய்தார்கள்.',\n",
       " 'தாய்த் திருநாட்டில் தங்களுக்குப் பிற்காலத்தில் வாழையடி வாழையாக வரப்போகும் ஆயிரங்கால சந்ததிகளுக்கும் நன்மை பயக்கும் மாபெரும் செயல்களை நிறைவேற்றி விட்டுப் போனார்கள் அல்லவா.',\n",
       " 'ஆடித் திங்கள் பதினெட்டாம் நாள் முன் மாலை நேரத்தில் அலை கடல் போல் விரிந்து பரந்திருந்த வீர நாராயண ஏரிக்கரை மீது ஒரு வாலிப வீரன் குதிரை ஏறிப் பிரயாணம் செய்து கொண்டிருந்தான்.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc73fa",
   "metadata": {},
   "source": [
    "We are going to tokenize the text and add start and end tokens:\n",
    "\n",
    "- <b>Tokenize the text</b>:\n",
    "        We’ll split the text into sentences using tokenize.sent_tokenize() and count the total number of sentences.\n",
    "\n",
    "- <b>Add start and end tokens:</b>\n",
    "        For each sentence, we’ll add SENTENCE_START at the beginning and SENTENCE_END at the end to help the model understand sentence boundaries.\n",
    "\n",
    "- <b>Example output:</b>\n",
    "        We’ll print the first 10 tokenized sentences to verify that everything is working as expected.\n",
    "\n",
    "\n",
    "Now we are going to clean and tokenize the sentences:\n",
    "\n",
    "- <b>Remove unwanted punctuation:</b>\n",
    "        We’ll remove periods from the tokenized sentences while keeping the sentence boundaries intact. This ensures we don’t lose important punctuation like SENTENCE_START and SENTENCE_END.\n",
    "- <b>Count word frequencies:</b>\n",
    "        Using Counter, we’ll count how often each word appears in the text and print the total number of unique word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ad2aca-b19e-4450-8105-ba14493e8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21129 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenize and build vocabulary\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Add SENTENCE_START and SENTENCE_END tokens\n",
    "tokenized_sentences = [[sentence_start_token] + sentence + [sentence_end_token] for sentence in tokenized_sentences]\n",
    "# Flatten tokenized sentences to get all words\n",
    "all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_words)\n",
    "print(f\"Found {len(word_freq)} unique word tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ed06bd5-59a3-4e51-a39d-05ff90ca95e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected vocabulary size: 16137 with 95.0% coverage\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocabulary to cover 95% of the text\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "total_word_count = sum(word_freq.values())\n",
    "coverage = 0\n",
    "vocab_size = 0\n",
    "desired_coverage = 0.95\n",
    "for word, count in word_freq.most_common():\n",
    "    coverage += count / total_word_count\n",
    "    vocab_size += 1\n",
    "    if coverage >= desired_coverage:\n",
    "        break\n",
    "\n",
    "print(f\"Selected vocabulary size: {vocab_size} with {desired_coverage * 100}% coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1d556",
   "metadata": {},
   "source": [
    "1. We sort the words by frequency and select the most common ones to cover 95% of the total word occurrences, determining the vocabulary size.  \n",
    "2. Mappings (`index_to_word` and `word_to_index`) are created for the vocabulary, including an `unknown_token` for out-of-vocabulary words.  \n",
    "3. Rare words in the tokenized sentences are replaced with `UNKNOWN_TOKEN` to ensure consistency during training.  \n",
    "4. An example sentence is shown with rare words replaced to verify how sentences look after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360a9be6-ebfa-430a-b0df-d936f702454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence after replacing rare words: ['SENTENCE_START', 'பொன்னியின்', 'செல்வன்', 'வரலாற்றுப்', 'புதினம்', 'அமரர்', 'கல்கி', 'கிருஷ்ணமூர்த்தி', 'அத்தியாயம்', 'ஆடித்திருநாள்', 'ஆதி', 'அந்தமில்லாத', 'கால', 'வெள்ளத்தில்', 'கற்பனை', 'ஓடத்தில்', 'ஏறி', 'நம்முடன்', 'சிறிது', 'நேரம்', 'பிரயாணம்', 'செய்யுமாறு', 'நேயர்களை', 'அழைக்கிறோம்', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "# Create mappings from word to index and index to word\n",
    "vocab = word_freq.most_common(vocab_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = {word: i for i, word in enumerate(index_to_word)}\n",
    "\n",
    "# Replace words not in the vocabulary with UNKNOWN_TOKEN\n",
    "tokenized_sentences = [[word if word in word_to_index else unknown_token for word in sentence] for sentence in tokenized_sentences]\n",
    "tokenized_sentences = [\n",
    "    [word for word in sentence if word != '.']\n",
    "    for sentence in tokenized_sentences\n",
    "]\n",
    "# Show an example sentence after rare word handling\n",
    "print(f\"Example sentence after replacing rare words: {tokenized_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8281d044-40e6-43a8-89cb-e92f82ec173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sentence: ['SENTENCE_START', 'என்னைப்', 'பலி', 'கொடுத்து', 'விடூங்கள்', 'SENTENCE_END']\n",
      "Sentence as indices: [1, 280, 775, 295, 1648, 2]\n",
      "Sentence from indices: ['SENTENCE_START', 'என்னைப்', 'பலி', 'கொடுத்து', 'விடூங்கள்', 'SENTENCE_END']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select a random sentence from tokenized_sentences\n",
    "random_sentence = random.choice(tokenized_sentences)\n",
    "\n",
    "# Convert the sentence to indices using word_to_index\n",
    "sentence_indices = [word_to_index[word] for word in random_sentence]\n",
    "\n",
    "# Convert the indices back to words using index_to_word\n",
    "sentence_words = [index_to_word[index] for index in sentence_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Random sentence:\", random_sentence)\n",
    "print(\"Sentence as indices:\", sentence_indices)\n",
    "print(\"Sentence from indices:\", sentence_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec69a9",
   "metadata": {},
   "source": [
    "## Step 3: Generating N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed461",
   "metadata": {},
   "source": [
    "Range of n-grams: We are harvesting all n-grams from bigrams (2-grams) to 20-grams by iterating through lengths from 2 to 20.\n",
    "\n",
    "Count n-grams: For each length i, the function ngrams() is used to generate all possible n-grams of that length from the text. The Counter is then used to count the occurrences of each n-gram.\n",
    "\n",
    "Store n-grams: The counts for each n-gram length are printed and stored in the ngrams_up_to_20 list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bcdd4e6-eb11-4923-ae41-9cfe374e3f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram-2 length: 326762\n",
      "ngram-3 length: 416139\n",
      "ngram-4 length: 427866\n",
      "ngram-5 length: 429198\n",
      "ngram-6 length: 429397\n",
      "ngram-7 length: 429440\n",
      "ngram-8 length: 429454\n",
      "ngram-9 length: 429456\n",
      "ngram-10 length: 429457\n",
      "ngram-11 length: 429457\n",
      "ngram-12 length: 429456\n",
      "ngram-13 length: 429455\n",
      "ngram-14 length: 429454\n",
      "ngram-15 length: 429453\n",
      "ngram-16 length: 429452\n",
      "ngram-17 length: 429451\n",
      "ngram-18 length: 429450\n",
      "ngram-19 length: 429449\n",
      "ngram-20 length: 429448\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Harvesting all n-grams up to length 20\n",
    "ngrams_up_to_20 = []\n",
    "for i in range(2, 21):\n",
    "    ngram_counts = Counter(ngrams(text.split(), i))  # Collecting n-grams of length i\n",
    "    print(f'ngram-{i} length:', len(ngram_counts))\n",
    "    ngrams_up_to_20.append(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670c1aa",
   "metadata": {},
   "source": [
    "We need to ensure that the n-grams we keep are complete and not broken by sentence-ending punctuation. So, we will implement helper functions to enure that. They are:\n",
    "- remove_periods(): This function checks if any word in the n-gram contains a period or quotation mark. If any such characters are found, the function returns False, indicating that the n-gram should be excluded.\n",
    "\n",
    "- my_filter(): This function applies remove_periods() to a list of n-grams, filtering out any n-grams that span sentence boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b275f1-9f28-4ed9-b48a-55de10392a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove n-grams that contain periods or quotes\n",
    "def remove_periods(ngram):\n",
    "    \"\"\"Remove n-grams that contain periods or quotes.\"\"\"\n",
    "    for word in ngram[0]:\n",
    "        if '.' in word or '’' in word or '‘' in word:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Keep only repeating n-grams\n",
    "def my_filter(ngrams):\n",
    "    \"\"\"Filter n-grams to only keep those that occur more than once and do not span sentence boundaries.\"\"\"\n",
    "    return filter(remove_periods, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47597373",
   "metadata": {},
   "source": [
    "## Step 4: Creating the Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9fadbe",
   "metadata": {},
   "source": [
    "Now, we'll construct the training dataset (X_train and y_train) using n-grams from 2-grams to 20-grams:\n",
    "\n",
    "- Initialize training data: Empty lists X_train and y_train are created to store the input sequences and target words.\n",
    "\n",
    "- Process n-grams: For each set of n-grams (from bigrams to 20-grams), we iterate through the most common n-grams.\n",
    "\n",
    "- Filter valid n-grams: Using my_filter(), we ensure all n-grams pass certain conditions, and only those where all words are in the vocabulary (word_to_index) are considered.\n",
    "\n",
    "- Create training examples:\n",
    "        X_train: The input sequence consists of the n-gram minus the last word.\n",
    "        y_train: The target is the last word of the n-gram.\n",
    "\n",
    "- Final count: The total number of sequences generated from n-grams is printed, showing how many training examples were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532fcee3-dc59-4348-a6ac-e239de8f4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences from n-grams: 340256\n"
     ]
    }
   ],
   "source": [
    "# Initialize training data lists\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Process all n-grams from 2 to 20\n",
    "for i in range(len(ngrams_up_to_20)):  # Starting from bigrams\n",
    "    ngrams_to_learn = ngrams_up_to_20[i]\n",
    "\n",
    "    # Construct X_train and y_train using the filtered n-grams\n",
    "    for sent in my_filter(ngrams_to_learn.most_common()):\n",
    "        ngram = sent[0]\n",
    "        # Ensure all words are in vocabulary\n",
    "        if all(word in word_to_index for word in ngram):\n",
    "            ngram_indices = [word_to_index[word] for word in ngram]\n",
    "            # Input sequence is the n-gram minus the last word\n",
    "            X_train.append(ngram_indices[:-1])\n",
    "            # Target is the last word of the n-gram\n",
    "            y_train.append(ngram_indices[-1])\n",
    "\n",
    "print(f'Total sequences from n-grams: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc465e2",
   "metadata": {},
   "source": [
    "We now expand the training dataset by incorporating sequences from complete tokenized sentences. For each sentence, we ensure that all words are in the vocabulary, and then we create input sequences using the words leading up to the current word, with the current word as the target output. This allows the model to learn from entire sentence structures, improving its ability to predict the next word based on the broader context of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc62717-31f6-4fe9-847c-2ffe32bb5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences after including sentences: 421996\n"
     ]
    }
   ],
   "source": [
    "# Include sequences from your tokenized sentences\n",
    "for sentence in tokenized_sentences:\n",
    "    if all(word in word_to_index for word in sentence):\n",
    "        sentence_indices = [word_to_index[word] for word in sentence]\n",
    "        for i in range(1, len(sentence_indices)):\n",
    "            X_train.append(sentence_indices[:i])\n",
    "            y_train.append(sentence_indices[i])\n",
    "\n",
    "print(f'Total sequences after including sentences: {len(X_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c164af",
   "metadata": {},
   "source": [
    "We combine X_train (input sequences) and y_train (target words) into a single list of tuples, then shuffle them together using random.shuffle().\n",
    "After shuffling, we unpack the combined list back into X_train and y_train, keeping the pairs aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d42a0a1-a1ff-44d5-af6b-d490581c3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "combined = list(zip(X_train, y_train))\n",
    "random.shuffle(combined)\n",
    "X_train[:], y_train[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4366af",
   "metadata": {},
   "source": [
    "we are preparing the training data by padding the sequences and converting them into the appropriate format for model training:\n",
    "\n",
    "- Determine maximum sequence length: We calculate the length of the longest sequence in X_train to use this as the standard for padding all sequences to the same length.\n",
    "\n",
    "- Pad sequences: Using pad_sequences(), we pad the input sequences (X_train) with zeros at the beginning (pre-padding). This ensures that all sequences have the same length, which is necessary for training models that expect fixed-length input.\n",
    "\n",
    "- Convert y_train to a NumPy array: We convert y_train to a NumPy array, making it compatible with machine learning libraries that require data in this format.\n",
    "\n",
    "- Print shape of padded data: We print the shape of the padded X_train and y_train to verify that they are ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d592d56-b426-4105-afc9-7baecace42a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 75\n",
      "X_train_padded shape: (421996, 75), y_train shape: (421996,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_seq_length = max(len(seq) for seq in X_train)\n",
    "print(f'Max sequence length: {max_seq_length}')\n",
    "\n",
    "# Pad sequences with zeros at the beginning\n",
    "X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# Convert y_train to a NumPy array\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f'X_train_padded shape: {X_train_padded.shape}, y_train shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ee5e2",
   "metadata": {},
   "source": [
    "We are splitting the dataset into training and validation sets. Using train_test_split(), 90% of the data is allocated for training, while 10% is reserved for validation. The validation set helps us assess the model's performance on unseen data, ensuring it generalizes well beyond the training set. The split is made reproducible by setting a random seed (random_state=42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "185ed9ee-9f55-45da-8bf2-b0e4e355983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (379796, 75), (379796,)\n",
      "Validation data shape: (42200, 75), (42200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train_padded_train, X_train_padded_val, y_train_train, y_train_val = train_test_split(\n",
    "    X_train_padded, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Training data shape: {X_train_padded_train.shape}, {y_train_train.shape}')\n",
    "print(f'Validation data shape: {X_train_padded_val.shape}, {y_train_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3be1854-89dd-4a3e-9809-42aace6296b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the processed data\n",
    "# with open('pickle/X_train_padded_ps.pkl', 'wb') as file:\n",
    "#     pickle.dump(X_train_padded, file)\n",
    "# with open('pickle/y_train_ps.pkl', 'wb') as file:\n",
    "#     pickle.dump(y_train, file)\n",
    "# with open('pickle/tokenized_sentences_ps.pkl', 'wb') as file:\n",
    "#     pickle.dump(tokenized_sentences, file)\n",
    "# with open('pickle/word_to_index_ps.pkl', 'wb') as file:\n",
    "#     pickle.dump(word_to_index, file)\n",
    "# with open('pickle/index_to_word_ps.pkl', 'wb') as file:\n",
    "#     pickle.dump(index_to_word, file)\n",
    "\n",
    "# print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b46d78-10be-4aad-a204-48016aae65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CBOW embeddings from your saved file\n",
    "# embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# # Open your CBOW embedding file\n",
    "# with open('my_cbow_vectors_ps.txt', 'r', encoding='utf-8') as f:\n",
    "#     # Read the header\n",
    "#     vocab_size, embedding_dim = map(int, f.readline().split())\n",
    "    \n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]  # The word itself\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "#         embeddings_index[word] = coefs\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Create the embedding matrix for the CBOW embeddings\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# for word, i in word_to_index.items():\n",
    "#     if i >= vocab_size:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "669fea16-54d1-4d32-b20f-a5c0457b7dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GloVe embeddings\n",
    "# embeddings_index = {}  # Initialize dictionary\n",
    "\n",
    "# # Open the GloVe embeddings file\n",
    "# with open('my_cbow_vectors_ps.txt', encoding='utf8') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]  # The word itself\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')  # The word vector\n",
    "#         embeddings_index[word] = coefs\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Create the embedding matrix\n",
    "# embedding_dim = 100\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# print(vocab_size)\n",
    "# for word, i in word_to_index.items():\n",
    "#     if i >= vocab_size:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d60a5",
   "metadata": {},
   "source": [
    "## Step 5: Building our Dense Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a62e1b",
   "metadata": {},
   "source": [
    "We define a Dense Neural Network (DNN) model using Keras Functional API to create Tamil embeddings:\n",
    "- Input Layer: An input layer is defined with a shape of (max_seq_length,) to receive the input sequences.\n",
    "- Embedding Layer: We use an embedding layer to create embeddings for the input data. This layer is trainable, with an output dimension of 100.\n",
    "- Flatten Layer: A flatten layer is used to reshape the embeddings, preparing them for the dense layers.\n",
    "- Dense Layer: A dense layer with 32 units and ReLU activation is added to capture features from the embeddings.\n",
    "- Output Layer: A dense output layer with softmax activation is used to predict the next word, matching the vocabulary size.\n",
    "- ReduceLROnPlateau: To improve convergence, ReduceLROnPlateau adjusts the learning rate when the loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b89774-db77-47d8-a799-abf5e6c59c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"dnn_embedding_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"dnn_embedding_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,613,700</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7500</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">240,032</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16137</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">532,521</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │     \u001b[38;5;34m1,613,700\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_layer (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7500\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_layer_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │       \u001b[38;5;34m240,032\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16137\u001b[0m)          │       \u001b[38;5;34m532,521\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,386,253</span> (9.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,386,253\u001b[0m (9.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,386,253</span> (9.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,386,253\u001b[0m (9.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Define the DNN model architecture using Functional API\n",
    "input_layer = Input(shape=(max_seq_length,), name=\"input_layer\")\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=100, name=\"embedding_layer\")(input_layer)\n",
    "flatten_layer = Flatten(name=\"flatten_layer\")(embedding_layer)\n",
    "dense_layer_1 = Dense(32, activation=\"relu\", name=\"dense_layer_1\")(flatten_layer)\n",
    "#dense_layer_2 = Dense(32, activation=\"relu\", name=\"dense_layer_2\")(dense_layer_1)\n",
    "output_layer = Dense(vocab_size, activation=\"softmax\", name=\"output_layer\")(dense_layer_1)\n",
    "\n",
    "# Create the model\n",
    "dnn_model = Model(inputs=input_layer, outputs=output_layer, name=\"dnn_embedding_model\")\n",
    "\n",
    "# Compile the model\n",
    "dnn_model.compile(optimizer=Adam(), loss=\"sparse_categorical_crossentropy\")\n",
    "dnn_model.summary()\n",
    "\n",
    "# Define the learning rate reduction callback\n",
    "reduce_lr_dnn = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67aafdc",
   "metadata": {},
   "source": [
    "## Step 6: Training our DNN Model on Ponniyn Selvan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b958b8bf",
   "metadata": {},
   "source": [
    "We are training the model using the fit method, where the training data (X_train_padded_train and y_train_train) is used to adjust the model's weights over 10 epochs, with a batch size of 128. The validation data (X_train_padded_val and y_train_val) is used to monitor the model's performance on unseen data during training. The ReduceLROnPlateau callback reduces the learning rate if the loss stops improving, helping the model converge more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d41ba32-292e-4f37-ba05-9f62f1c3f2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12ms/step - loss: 8.2285 - val_loss: 7.6398 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12ms/step - loss: 7.4120 - val_loss: 7.1715 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12ms/step - loss: 6.8089 - val_loss: 6.7289 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11ms/step - loss: 6.2253 - val_loss: 6.3411 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 12ms/step - loss: 5.6429 - val_loss: 6.0815 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12ms/step - loss: 5.1697 - val_loss: 5.9020 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12ms/step - loss: 4.8091 - val_loss: 5.8232 - learning_rate: 0.0010\n",
      "Epoch 8/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12ms/step - loss: 4.5342 - val_loss: 5.7610 - learning_rate: 0.0010\n",
      "Epoch 9/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12ms/step - loss: 4.3156 - val_loss: 5.7405 - learning_rate: 0.0010\n",
      "Epoch 10/10\n",
      "\u001b[1m2968/2968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12ms/step - loss: 4.1223 - val_loss: 5.7576 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history_dnn = dnn_model.fit(\n",
    "    X_train_padded_train, y_train_train,\n",
    "    validation_data=(X_train_padded_val, y_train_val),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    callbacks=[reduce_lr_dnn]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c27a4",
   "metadata": {},
   "source": [
    "## Step 7: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712cdc59-b00e-46e5-85d0-f8c1ecd4611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights using pickle\n",
    "# with open('tensorflow_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "dnn_model.save('model/dnn_model.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb58a8",
   "metadata": {},
   "source": [
    "## Step 8: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c7e82f",
   "metadata": {},
   "source": [
    "#### Generate Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3251b",
   "metadata": {},
   "source": [
    "The generate_sentence() function uses the trained DNN model to generate sentences. It starts with the sentence_start_token and predicts each subsequent word by sampling from the model's probability distribution for the next word. The generation continues until it reaches the sentence_end_token or a specified sentence length. The predicted word indices are then converted back into actual words, forming a complete sentence. This function enables the model to create coherent text based on the patterns it has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b363eed4-fcbb-4c56-85cb-f5a1fb180687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, word_to_index, index_to_word, max_seq_length, senten_max_length):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the maximum sentence length\n",
    "    while (new_sentence[-1] != word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word, avoiding UNKNOWN_TOKEN\n",
    "        sampled_word_index = word_to_index[unknown_token]\n",
    "        while sampled_word_index == word_to_index[unknown_token]:\n",
    "            # Sample from the distribution\n",
    "            sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Append the sampled word to the sentence\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices to words, excluding SENTENCE_START and SENTENCE_END tokens\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence[1:-1]]\n",
    "    return ' '.join(sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df767a57-c3a4-4036-8f7b-2f658931a991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "இதற்குக் ஒரு தவறைச் செய்த ஆழ்வார்க்கடியான் ஒருவாறு தனமும்\n",
      "திக்குத் பொன் கூரை அமைந்த சிற்றரசர்கள் அவரைத் வந்து அது தான் _ comma _ போல் சொல்லுங்கள் _ comma _ தூண்கள்\n",
      "ஓடிப் போவதில் தெரியாத செய்தி அவ்வளவு பதில் உண்டா\n",
      "நடந்தார் என்று சொல்ல முடியாமல் கூடத் கேட்டூத் இதில் வந்து அவன் எண்ணிக் கொண்டது உண்மைதான்\n",
      "சரியாகப் பிடித்துக் கொண்டு பறந்து செல்ல கொண்டு காதில் முயன்றான்\n",
      "அங்கே _ comma _ நான் எல்லாவற்றையும் போக எப்படி கேட்கப் போக வேண்டுமென்று வந்தாய்\n",
      "பேசக் கூடாது என்று அவர் சொன்னான் சொல்லிக் கொண்டுதான் உள்ளானது கொடுத்துக் கொண்டது பண்ணிக் கொண்டு பூசாரிகள் கரகம் எடுத்து ஆடிக் கொண்டும் உடுக்கு\n",
      "சொல்கிறேன் “நீ தனாதிகாரி பெரிய சம்புவரையர் பழையாறையிலும் எங்கும் திகைத்து கூடி ஒரு கூச்சல்\n",
      "யானை _ comma _ மூன்று பேரில் மாளிகையில் இப்போது முன்னதாகவே வந்து ஊராய்ப் வந்தியத்தேவன் பெருமை விழுந்து பாய்ந்தான்\n",
      "உன் பள்ளிப்படை புறப்பட்ட இடத்துக்கே ஒரு பிரயத்தனம் நேராமல் பார்த்துக் கொண்டார் _ comma _ இராஜாதித்யன் குதிரை அருகில் இருந்த வரையில் இம்மாதிரி\n",
      "என்னிடம் அவனுக்கு என்ன பேசிய சமயத்தில் ஆழ்வார்க்கடியான் ஒரு தடவை அக்காலத்தில் _ comma _ எனக்கு என்ன தெரியும்\n",
      "“குதிரையை நான் கூறி அருகில் வந்து இறங்க வேண்டும் என்று கிடையாது\n",
      "என்னவென்பது நம் வீராதி வீரன் உண்டு என்பதை முன்னமே வாசகர்களுக்கு நாம் தெரிந்து கொண்டூ சாலையோடூ சென்ற இடத்துக்குச் பிறகு தூரத்தில் பின்னர் சிலர்\n",
      "” என்று கூறி கண் மேல் எங்கேயோ பதிந்து சுற்றிச் சுற்றிப் பார்த்துவிட்டு பேசும் சொன்னான் _ comma _ அவர்கள் வந்து விடுவாய்\n",
      "அவசரமாக இந்தக் கூட்டத்தாரின் ஆலோசனையில் கலந்து கொண்டிருக்கிறான் குறித்துப் மகான் சப்தமும் தூக்கிக் கொள்வார்கள் என்றும் வந்தேன் என்று மனக் சபதம் போல் வைத்துக்\n",
      "” என்று உங்கள் முகத்தில் தோன்றிய தெரியாதா _ comma _ நான்\n",
      "எப்படியிருந்தாலும் உன் மாளிகையின் மருத்துவர் விரலை கேட்டுப் மேல் மோதிக் கொண்டார்கள் பார்த்தான்\n",
      "” எத்தனையோ பெண்கள் என்னைக் மயில் பாடல் சுந்தர சோழரின் சக்கரவர்த்தி இருந்தது\n",
      "அவனுடைய பின்னாலேயே என்னைப் பிடித்துக் கொண்டூ வந்தார்கள் என்றும் கூறியது குற்றம்\n",
      "அவசரமாக போனால் நான் எப்படி விட்டுப் போகும்படிதான் இளையபிராட்டி தங்கள் கடல் காப்பாற்ற அவன் அந்த உடல் உயிர் எடூத்துக் கொண்டூ ஓடுவது ”\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 20\n",
    "senten_min_length = 7\n",
    "senten_max_length = 20\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    sent = ''\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent.split()) < senten_min_length:\n",
    "        sent = generate_sentence(dnn_model, word_to_index, index_to_word, max_seq_length, senten_max_length)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c1bd5",
   "metadata": {},
   "source": [
    "#### Generating based on a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36a895",
   "metadata": {},
   "source": [
    "The generate_sentence() function generates a sentence based on a given starting text. It first converts the starting words into word indices using the model's vocabulary. Then, it uses the trained RNN model to predict and append the next word, continuing until the sentence reaches a specified maximum length or the end token is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4735a82-d61b-4751-8e6c-12c7b8e49b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_with_start(model, word_to_index, index_to_word, max_seq_length, start_text, senten_max_length):\n",
    "    # Convert the start_text to lowercase to match your vocabulary\n",
    "    start_text = start_text.lower()\n",
    "    \n",
    "    # Convert the start_text into indices based on your vocabulary\n",
    "    new_sentence = [word_to_index.get(word, word_to_index[unknown_token]) for word in start_text.split()]\n",
    "    \n",
    "    # Repeat until we get an end token or reach the max sentence length\n",
    "    while len(new_sentence) < senten_max_length:\n",
    "        # Prepare the input sequence\n",
    "        sequence = new_sentence\n",
    "        # Pad the sequence\n",
    "        sequence_padded = pad_sequences([sequence], maxlen=max_seq_length, padding='pre')\n",
    "        \n",
    "        # Predict the next word\n",
    "        predicted_probs = model.predict(sequence_padded, verbose=0)[0]\n",
    "        # Get the probabilities for the last time step\n",
    "        next_word_probs = predicted_probs\n",
    "        \n",
    "        # Sample the next word\n",
    "        sampled_word_index = np.random.choice(len(next_word_probs), p=next_word_probs)\n",
    "        \n",
    "        # Check if the sampled word is the SENTENCE_END token\n",
    "        if sampled_word_index == word_to_index.get(sentence_end_token):\n",
    "            break  # Stop adding words if we reach the end token\n",
    "        \n",
    "        # Append the predicted word\n",
    "        new_sentence.append(sampled_word_index)\n",
    "    \n",
    "    # Convert indices back to words\n",
    "    sentence_str = [index_to_word[idx] for idx in new_sentence]\n",
    "    generated_text = ' '.join(sentence_str)\n",
    "    generated_text += '.'\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96713e",
   "metadata": {},
   "source": [
    "#### <b> Prompts and their Outputs </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "184879e0-c9aa-498e-977f-ba257808d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: பொதுவாக தங்கள் மூலையில் அங்குமிங்கும் நின்று எந்த நகரம் அடிக்கடி இப்போது நாலு ஏரியில் குதித்து நிற்பது அதன் பொருள்கள் என்பதைக் சிற்ப கையை நெருங்கி பிடுங்கித் காலில் சுற்றி மேடை மீது நின்று வேல் UNKNOWN_TOKEN UNKNOWN_TOKEN _ comma _ ஆச்சார்ய _ comma _ சிந்தி _ comma _ மோதிரத்தைக் என்று _ comma _ மகாராஜ்யத்தின் _ comma _ முடியாது முதலிய.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"பொதுவாக\"\n",
    "generated_output = generate_sentence_with_start(dnn_model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74b86753-13c3-4012-ae10-8828212527d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: வணக்கம் செய்துவிட்டுப் _ comma _ காரியங்கள் மொழி உத்தேசம் எனக்கு தம் காலில் மாடத்துக்கு வந்து கொண்டிருந்தார் சிறிது சொன்னான் சொல்ல வேண்டியது UNKNOWN_TOKEN திருப்தி செய்தீர்கள்.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"வணக்கம்\"\n",
    "generated_output = generate_sentence_with_start(dnn_model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6acfcf06-f6ad-4536-b3c1-0a2621706b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: சோழன் புதல்வியான ரதங்களின் கொம்பில் அத்தகைய வெறியாட்டுக்கும் பழைய _ comma _ < / div > என்று மூன்று சாம்ராஜ்யம் உள்ள நல்ல கர்ஜனை செய்தான்.\n"
     ]
    }
   ],
   "source": [
    "start_prompt = \"சோழன்\"\n",
    "generated_output = generate_sentence_with_start(dnn_model, word_to_index, index_to_word, max_seq_length, start_prompt, senten_max_length=50)\n",
    "\n",
    "print(\"Generated text:\", generated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a95e6",
   "metadata": {},
   "source": [
    "## Step 9: Saving the Model Weights, as they act as our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5f3f6bc-ef7d-451d-a2db-70180d79dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the model weights using pickle\n",
    "# with open('tensorflow_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "\n",
    "dnn_model.save_weights('model/dnn_model_weight.weights.h5')\n",
    "print('Model Saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
